[
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html",
    "title": "Take-Home_Ex02",
    "section": "",
    "text": "Thailand’s tourism industry plays a crucial role in the country’s economy, contributing around 20% to its GDP. In 2019, tourism revenues reached 90 billion US$, but the COVID-19 pandemic drastically reduced this to 24 billion US$ in 2020. Since September 2021, the sector has shown signs of gradual recovery. However, the tourism economy is not uniformly distributed across Thailand; the majority of tourism activities and revenues are concentrated in five key provinces: Bangkok, Phuket, Chiang Mai, Sukhothai, and Phetchaburi. These regions have been the focus of recovery and continue to drive Thailand’s tourism sector post-pandemic.\n\n\n\nExamine Spatial and Spatio-temporal Dependence:\n\nInvestigate if Thailand’s tourism economy indicators are dependent on space and time.\n\nDetect Clusters and Outliers:\n\nIdentify spatial clusters and outliers in tourism activity across provinces.\n\nIdentify Emerging Hotspots/Coldspots:\n\nDetect regions with increasing or decreasing tourism activity over time.\n\nPrepare Geospatial Data:\n\nCreate geospatial data layers for provinces, tourism indicators, and a spacetime tourism indicator layer.\n\nPerform Spatial Analysis:\n\nConduct global and local spatial autocorrelation, and emerging hotspot analysis using sfdep methods.\n\n\n\n\n\nThe focus of this study would be in Thailand.\n\n\n\nThe code chunk below installs and loads sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate, dbscan, igraph, dodgr,future,spacetime, sfdep,cowplot,spdep, packages into R environment. \n\npacman::p_load(sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate, dbscan, igraph, dodgr,future,spacetime, sfdep,cowplot,spdep)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#get-started",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#get-started",
    "title": "Take-Home_Ex02",
    "section": "",
    "text": "Examine Spatial and Spatio-temporal Dependence:\n\nInvestigate if Thailand’s tourism economy indicators are dependent on space and time.\n\nDetect Clusters and Outliers:\n\nIdentify spatial clusters and outliers in tourism activity across provinces.\n\nIdentify Emerging Hotspots/Coldspots:\n\nDetect regions with increasing or decreasing tourism activity over time.\n\nPrepare Geospatial Data:\n\nCreate geospatial data layers for provinces, tourism indicators, and a spacetime tourism indicator layer.\n\nPerform Spatial Analysis:\n\nConduct global and local spatial autocorrelation, and emerging hotspot analysis using sfdep methods."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#study-area",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#study-area",
    "title": "Take-Home_Ex02",
    "section": "",
    "text": "The focus of this study would be in Thailand."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#install-and-launching-r-packages.",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#install-and-launching-r-packages.",
    "title": "Take-Home_Ex02",
    "section": "",
    "text": "The code chunk below installs and loads sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate, dbscan, igraph, dodgr,future,spacetime, sfdep,cowplot,spdep, packages into R environment. \n\npacman::p_load(sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate, dbscan, igraph, dodgr,future,spacetime, sfdep,cowplot,spdep)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#import-data",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#import-data",
    "title": "Take-Home_Ex02",
    "section": "3.1 Import Data",
    "text": "3.1 Import Data\n\nLoading the CSV File: The dataset \"thailand_domestic_tourism_2019_2023_ver2.csv\" located in the path \"data/rawdata\" is read into R using the read_csv function from the readr package. This file is assumed to contain data on domestic tourism activity in Thailand between the years 2019 and 2023.\nSelecting Specific Columns: The dataset thai_tourism_data is then modified using the select function from dplyr to remove the columns province_thai and region_thai. These columns presumably represent the province names and region names in Thai script, and the user is excluding them, likely to focus on the English versions or other variables.\n\n\nthai_tourism_data &lt;- read_csv(\"data/rawdata/thailand_domestic_tourism_2019_2023_ver2.csv\")\nthai_tourism_data &lt;- thai_tourism_data %&gt;% select(-province_thai, -region_thai)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-tourist-data-by-year-province-and-region-in-thailand",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-tourist-data-by-year-province-and-region-in-thailand",
    "title": "Take-Home_Ex02",
    "section": "3.2 Aggregating and Summarizing Tourist Data by Year, Province, and Region in Thailand",
    "text": "3.2 Aggregating and Summarizing Tourist Data by Year, Province, and Region in Thailand\n\nFiltering the Dataset:\n\nThe code filters the thai_tourism_data dataset for records where the variable is equal to \"no_tourist_all\". This variable likely represents the total number of tourists in each record, isolating only the relevant data for this analysis.\n\nExtracting the Year:\n\nThe mutate() function creates a new column, year, which extracts the year component from the date column. This enables the dataset to be grouped and analyzed on a yearly basis.\n\nGrouping Data:\n\nThe dataset is grouped by several attributes: year, province_eng (province name in English), region_eng (region name in English), and variable. This allows for the aggregation of tourist data across years, provinces, and regions.\n\nSummarizing the Total Number of Tourists:\n\nThe summarise() function computes the total number of tourists by summing the values in the value column for each group (year, province, region, variable). The na.rm = TRUE option ensures that any missing values are ignored during the summation.\n\n\n\nno_tourist_all &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"no_tourist_all\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-foreign-tourist-data-by-year-province-and-region-in-thailand",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-foreign-tourist-data-by-year-province-and-region-in-thailand",
    "title": "Take-Home_Ex02",
    "section": "3.3 Aggregating and Summarizing Foreign Tourist Data by Year, Province, and Region in Thailand",
    "text": "3.3 Aggregating and Summarizing Foreign Tourist Data by Year, Province, and Region in Thailand\n\nFiltering the Dataset:\n\nThe code filters the thai_tourism_data dataset for rows where the variable is equal to \"no_tourist_foreign\". This variable likely represents the number of foreign tourists, isolating data that pertains specifically to international tourism.\n\nExtracting the Year:\n\nUsing the mutate() function, the year column is created by extracting the year from the date column. This allows for the aggregation of data on an annual basis.\n\nGrouping Data:\n\nThe dataset is grouped by year, province_eng, region_eng, and variable to structure the data based on these dimensions, making it easier to calculate summaries for each combination of year, province, and region.\n\nSummarizing the Total Number of Foreign Tourists:\n\nThe summarise() function computes the total number of foreign tourists by summing the values in the value column for each group. Missing values (NA) are ignored using na.rm = TRUE.\n\n\n\nno_tourist_foreign &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"no_tourist_foreign\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-tourist-stay-data-by-year-province-and-region-in-thailand",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-tourist-stay-data-by-year-province-and-region-in-thailand",
    "title": "Take-Home_Ex02",
    "section": "3.4 Aggregating and Summarizing Tourist Stay Data by Year, Province, and Region in Thailand",
    "text": "3.4 Aggregating and Summarizing Tourist Stay Data by Year, Province, and Region in Thailand\n\nFiltering the Dataset:\n\nThe thai_tourism_data dataset is filtered for rows where the variable is equal to \"no_tourist_stay\". This variable likely refers to the number of tourists staying in certain locations, isolating data related to overnight stays or similar metrics.\n\nExtracting the Year:\n\nThe mutate() function is used to extract the year from the date column and store it in a new column called year. This prepares the data for aggregation by year.\n\nGrouping Data:\n\nThe group_by() function groups the data by year, province_eng (the province in English), region_eng (the region in English), and variable. This organizes the data for each unique combination of year, province, and region.\n\nSummarizing the Total Tourist Stay Values:\n\nThe summarise() function calculates the total number of tourists staying (or related statistic) by summing the values in the value column. The na.rm = TRUE argument ensures that missing values (NA) are ignored in the summation process.\n\n\n\nno_tourist_stay &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"no_tourist_stay\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-domestic-thai-tourist-data-by-year-province-and-region",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-domestic-thai-tourist-data-by-year-province-and-region",
    "title": "Take-Home_Ex02",
    "section": "3.5 Aggregating and Summarizing Domestic Thai Tourist Data by Year, Province, and Region",
    "text": "3.5 Aggregating and Summarizing Domestic Thai Tourist Data by Year, Province, and Region\n\nFiltering the Dataset:\n\nThe thai_tourism_data dataset is filtered to include only rows where the variable is equal to \"no_tourist_thai\", which represents the number of Thai tourists traveling within the country. This isolates data specifically related to domestic tourism.\n\nExtracting the Year:\n\nThe mutate() function is used to create a new year column by extracting the year from the date column. This allows for the aggregation of data on a yearly basis.\n\nGrouping Data:\n\nThe group_by() function groups the data by year, province_eng (province name in English), region_eng (region name in English), and variable. This groups the data by time and location to summarize domestic tourist activity in each province and region over time.\n\nSummarizing the Total Number of Domestic Tourists:\n\nThe summarise() function calculates the total number of domestic tourists by summing the value column for each group. Missing values are ignored with the na.rm = TRUE argument.\n\n\n\nno_tourist_thai &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"no_tourist_thai\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-average-ratio-of-tourist-stays-by-year-province-and-region",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-the-average-ratio-of-tourist-stays-by-year-province-and-region",
    "title": "Take-Home_Ex02",
    "section": "3.6 Calculating the Average Ratio of Tourist Stays by Year, Province, and Region",
    "text": "3.6 Calculating the Average Ratio of Tourist Stays by Year, Province, and Region\n\nFiltering the Dataset:\n\nThe dataset thai_tourism_data is filtered to include only rows where the variable is equal to \"ratio_tourist_stay\". This variable likely represents the ratio of tourist stays, such as the proportion of tourists staying overnight or a similar measure.\n\nExtracting the Year:\n\nThe mutate() function creates a new column year by extracting the year component from the date column. This allows for the data to be analyzed on an annual basis.\n\nGrouping Data:\n\nThe group_by() function groups the data by year, province_eng, region_eng, and variable. This prepares the data for calculating summaries, grouping it by time and location.\n\nCalculating the Average Ratio:\n\nThe summarise() function calculates the average value of the ratio using mean() for each group. The na.rm = TRUE argument ensures that missing values are ignored during the calculation of the mean.\n\n\n\nratio_tourist_stay &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"ratio_tourist_stay\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(avg_value = mean(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-total-tourism-revenue-by-year-province-and-region",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-total-tourism-revenue-by-year-province-and-region",
    "title": "Take-Home_Ex02",
    "section": "3.7 Aggregating and Summarizing Total Tourism Revenue by Year, Province, and Region",
    "text": "3.7 Aggregating and Summarizing Total Tourism Revenue by Year, Province, and Region\n\nFiltering the Dataset:\n\nThe dataset thai_tourism_data is filtered to include only rows where the variable is equal to \"revenue_all\". This variable likely represents the total tourism revenue, isolating records specifically related to revenue.\n\nExtracting the Year:\n\nThe mutate() function is used to extract the year from the date column and create a new column called year. This prepares the data for aggregation by year.\n\nGrouping Data:\n\nThe data is grouped by year, province_eng (province name in English), region_eng (region name in English), and variable. This groups the data based on time and location to organize the revenue data across different provinces and regions.\n\nSummarizing Total Revenue:\n\nThe summarise() function calculates the total revenue by summing the value column for each group (year, province, region, variable). The na.rm = TRUE argument ensures that missing values are ignored during the summation.\n\n\n\nrevenue_all &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"revenue_all\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-foreign-tourism-revenue-by-year-province-and-region",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-foreign-tourism-revenue-by-year-province-and-region",
    "title": "Take-Home_Ex02",
    "section": "3.8 Aggregating and Summarizing Foreign Tourism Revenue by Year, Province, and Region",
    "text": "3.8 Aggregating and Summarizing Foreign Tourism Revenue by Year, Province, and Region\n\nFiltering the Dataset:\n\nThe dataset thai_tourism_data is filtered to include only rows where the variable is equal to \"revenue_foreign\". This variable likely represents the revenue generated from foreign tourists, isolating relevant data for foreign tourism revenue.\n\nExtracting the Year:\n\nThe mutate() function creates a new column year by extracting the year from the date column. This step allows for summarizing data by year.\n\nGrouping Data:\n\nThe group_by() function groups the data by year, province_eng (province name in English), region_eng (region name in English), and variable. This groups the data by time and location to organize the foreign revenue data for each province and region.\n\nSummarizing Total Foreign Tourism Revenue:\n\nThe summarise() function calculates the total foreign tourism revenue by summing the value column for each group. Missing values are ignored using the na.rm = TRUE argument.\n\n\n\nrevenue_foreign &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"revenue_foreign\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-domestic-tourism-revenue-by-year-province-and-region",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#aggregating-and-summarizing-domestic-tourism-revenue-by-year-province-and-region",
    "title": "Take-Home_Ex02",
    "section": "3.9 Aggregating and Summarizing Domestic Tourism Revenue by Year, Province, and Region",
    "text": "3.9 Aggregating and Summarizing Domestic Tourism Revenue by Year, Province, and Region\n\nFiltering the Dataset:\n\nThe thai_tourism_data dataset is filtered to include only rows where the variable is equal to \"revenue_thai\". This variable likely represents the revenue generated from domestic tourists (Thai nationals), allowing for focused analysis on domestic tourism revenue.\n\nExtracting the Year:\n\nThe mutate() function creates a new column year by extracting the year from the date column. This enables aggregation of the revenue data by year.\n\nGrouping Data:\n\nThe data is grouped by year, province_eng (province name in English), region_eng (region name in English), and variable. This grouping organizes the revenue data by time (year) and space (province and region).\n\nSummarizing Total Domestic Tourism Revenue:\n\nThe summarise() function calculates the total domestic tourism revenue by summing the value column for each group. The na.rm = TRUE argument ensures that missing values are ignored during the summation process.\n\n\n\nrevenue_thai &lt;- thai_tourism_data %&gt;%\n  filter(variable == \"revenue_thai\") %&gt;%\n  mutate(year = year(date)) %&gt;%\n  group_by(year, province_eng, region_eng, variable) %&gt;%\n  summarise(total_value = sum(value, na.rm = TRUE))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#reading-administrative-boundaries-using-sf-package-in-r",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#reading-administrative-boundaries-using-sf-package-in-r",
    "title": "Take-Home_Ex02",
    "section": "3.9 Reading Administrative Boundaries Using sf Package in R",
    "text": "3.9 Reading Administrative Boundaries Using sf Package in R\n\nFunction Used:\n\nThe function st_read() from the sf package is used to read the geospatial data. This function is commonly used to load shapefiles or other spatial data formats into R.\n\nData Source:\n\nThe argument dsn = \"data/rawdata\" specifies the data source name (the directory where the spatial data is located). In this case, it points to the folder containing the raw data files.\n\nLayer Specification:\n\nThe argument layer = \"tha_admbnda_adm1_rtsd_20220121\" specifies the specific layer to be read from the file. This is typically the name of the shapefile (without the .shp extension) or the desired layer within a data source containing multiple layers. This layer likely represents Thailand’s administrative boundaries at a certain level (e.g., provincial or regional).\n\n\n\nadmin_boundaries &lt;- st_read(dsn = \"data/rawdata\",\n                        layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Take-Home_Ex\\Take-Home_Ex02\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#selecting-and-removing-unnecessary-columns-from-administrative-boundaries-data",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#selecting-and-removing-unnecessary-columns-from-administrative-boundaries-data",
    "title": "Take-Home_Ex02",
    "section": "3.10 Selecting and Removing Unnecessary Columns from Administrative Boundaries Data",
    "text": "3.10 Selecting and Removing Unnecessary Columns from Administrative Boundaries Data\n\nData Preparation:\n\nThe code modifies the admin_boundaries dataset by selecting only relevant columns and removing others. The dataset contains administrative boundary data, and the goal is to focus on a smaller subset of the data.\n\nColumns Removed:\n\nThe following columns are excluded from the dataset:\n\nADM1_TH, ADM1_REF, ADM1_ALT1EN, ADM1_ALT1TH, ADM1_ALT2EN, ADM1_ALT2TH, ADMO_EN, ADMO_TH, ADMO_PCODE, date, validOn, validTo, Shape_Leng, Shape_Area, ADM1_PCODE.\n\nThese columns likely represent metadata or attributes that are either redundant or not needed for the upcoming analysis.\n\nPurpose:\n\nBy removing these columns, the dataset is simplified, making it easier to work with and reducing unnecessary complexity. This step prepares the dataset for further spatial analysis or visualization tasks by focusing only on the essential data.\n\n\n\nadmin_boundaries &lt;- admin_boundaries %&gt;% select(-ADM1_TH,-ADM1_REF,-ADM1ALT1EN,-ADM1ALT1TH,-ADM1ALT2EN,-ADM1ALT2TH,-ADM0_EN,-ADM0_TH,-ADM0_PCODE,-date,-validOn,-validTo,-Shape_Leng,-Shape_Area,-ADM1_PCODE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#renaming-columns-in-the-administrative-boundaries-data",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#renaming-columns-in-the-administrative-boundaries-data",
    "title": "Take-Home_Ex02",
    "section": "3.11 Renaming Columns in the Administrative Boundaries Data",
    "text": "3.11 Renaming Columns in the Administrative Boundaries Data\n\nRenaming a Column:\n\nThe column ADM1_EN, which likely represents the English names of provinces in Thailand, is being renamed to province_eng. This renaming improves clarity and consistency in the dataset by providing a more descriptive column name.\n\nPurpose:\n\nThe new column name province_eng more clearly conveys that this column contains the names of provinces in English. This is especially helpful for further analysis, making the data more readable and easier to interpret.\n\n\n\nadmin_boundaries &lt;- admin_boundaries %&gt;%\n  rename(\n    province_eng = ADM1_EN\n  )"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#joining-administrative-boundaries-with-tourism-and-revenue-data",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#joining-administrative-boundaries-with-tourism-and-revenue-data",
    "title": "Take-Home_Ex02",
    "section": "3.11 Joining Administrative Boundaries with Tourism and Revenue Data",
    "text": "3.11 Joining Administrative Boundaries with Tourism and Revenue Data\nThis R code snippet is used to merge multiple tourism-related datasets with the administrative boundaries dataset. The left_join() function from the dplyr package is used to merge these datasets based on the common column province_eng. Below is a breakdown of the process:\n\nDatasets to Be Merged:\n\nThe datasets include:\n\nno_tourist_all: Total number of tourists.\nno_tourist_foreign: Number of foreign tourists.\nno_tourist_stay: Number of tourists staying in the province.\nno_tourist_thai: Number of Thai (domestic) tourists.\nratio_tourist_stay: Ratio of tourist stays.\nrevenue_all: Total tourism revenue.\nrevenue_foreign: Revenue from foreign tourists.\nrevenue_thai: Revenue from domestic (Thai) tourists.\n\n\nJoining with Administrative Boundaries:\n\nEach of these datasets is merged (joined) with the admin_boundaries dataset using a left join, ensuring that all rows from admin_boundaries are retained and only matching rows from the tourism datasets are included.\nThe join is based on the province_eng column, which is the shared key in both the admin_boundaries dataset and each tourism dataset.\n\nPurpose:\n\nThis process integrates spatial data (administrative boundaries) with tourism and revenue data, making it possible to perform spatial analysis and visualize tourism trends and revenue generation across different provinces.\n\n\n\nno_tourist_all &lt;- admin_boundaries%&gt;%\n  left_join(no_tourist_all, by = \"province_eng\")\n\nno_tourist_foreign &lt;- admin_boundaries%&gt;%\n  left_join(no_tourist_foreign, by = \"province_eng\")\n\nno_tourist_stay &lt;- admin_boundaries%&gt;%\n  left_join(no_tourist_stay, by = \"province_eng\")\n\nno_tourist_thai &lt;- admin_boundaries%&gt;%\n  left_join(no_tourist_thai, by = \"province_eng\")\n\nratio_tourist_stay &lt;- admin_boundaries%&gt;%\n  left_join(ratio_tourist_stay, by = \"province_eng\")\n\nrevenue_all &lt;- admin_boundaries%&gt;%\n  left_join(revenue_all, by = \"province_eng\")\n\nrevenue_foreign &lt;- admin_boundaries%&gt;%\n  left_join(revenue_foreign, by = \"province_eng\")\n\nrevenue_thai &lt;- admin_boundaries%&gt;%\n  left_join(revenue_thai, by = \"province_eng\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-total-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-total-tourists",
    "title": "Take-Home_Ex02",
    "section": "4.1 Plotting a Map with Highlighted Top 5 Provinces by Total Tourists",
    "text": "4.1 Plotting a Map with Highlighted Top 5 Provinces by Total Tourists\n\nBase Map Plot:\n\nThe ggplot() function initializes the plot using the no_tourist_all dataset.\ngeom_sf() is used to plot the geometry of all provinces, with the fill representing total_value (total tourists) and a border color of “black”. The size of the borders is set to 0.2.\nfacet_wrap(~ year) is used to create a separate plot for each year, allowing for a yearly comparison of tourism data.\n\nHighlighting Top 5 Provinces:\n\nAnother geom_sf() layer is added to highlight the top 5 provinces with the highest number of tourists for each year. This is done by filtering the no_tourist_all dataset and using top_n(5, total_value) to select the top 5 provinces based on the total_value column.\nThe top 5 provinces are highlighted with a fill = \"red\" and a thicker border (size = 0.4).\n\nColor Scale for Other Provinces:\n\nThe scale_fill_viridis_c() function is applied to give the remaining provinces a color gradient based on the total number of tourists. The option = \"plasma\" defines the color scheme, and the name for the scale is set to \"Total Tourists\".\n\nTitles and Labels:\n\nThe labs() function is used to add a title, caption, and legend label to the plot. The title is set to indicate the total number of tourists by province and year, with a special note that the top 5 provinces are highlighted.\n\nClean Theme:\n\nThe theme_minimal() function is applied to give the plot a minimalistic and clean look.\nAdditional theme customizations are made to remove axis text, ticks, grids, and background by setting these elements to element_blank().\n\n\n\nggplot(no_tourist_all) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = no_tourist_all %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"no_tourist_all by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\nTourism Decline (2020-2021): Significant drop in tourist numbers, especially in the top provinces (marked in red), likely due to the COVID-19 pandemic.\nRecovery (2022-2023): Tourist numbers begin to recover, with an increase in red-highlighted provinces, but still below 2019 levels.\nConcentration: Key tourist provinces remain consistent across the years, indicating reliance on a few regions.\n\n\n\nAnalysis:\n\nCOVID-19 Impact: A clear decline in tourism during 2020-2021, with a gradual recovery from 2022 onwards.\nFocus on Key Provinces: Tourism remains concentrated in a few provinces, suggesting targeted recovery efforts should focus on these areas."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-foreign-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-foreign-tourists",
    "title": "Take-Home_Ex02",
    "section": "4.2 Plotting a Map with Highlighted Top 5 Provinces by Foreign Tourists",
    "text": "4.2 Plotting a Map with Highlighted Top 5 Provinces by Foreign Tourists\n\nBase Map Plot:\n\nThe ggplot() function initializes the plot using the no_tourist_foreign dataset.\ngeom_sf() is used to plot the geometry of all provinces, where the fill represents total_value (the total number of foreign tourists). The borders of the provinces are colored “black”, and the size of the borders is set to 0.2.\nThe map is faceted by year using facet_wrap(~ year), allowing separate maps for each year to compare trends in foreign tourists across years.\n\nHighlighting Top 5 Provinces:\n\nA second geom_sf() layer is added to highlight the top 5 provinces with the highest number of foreign tourists for each year. The top_n(5, total_value) function selects the top 5 provinces based on the total_value column.\nThese top 5 provinces are highlighted in red (fill = \"red\") with a thicker black border (size = 0.4), making them stand out on the map.\n\nColor Scale for Other Provinces:\n\nThe scale_fill_viridis_c() function is applied to give the rest of the provinces a color gradient based on the total number of foreign tourists. The option = \"plasma\" defines the color palette, and the legend is labeled as \"Total Tourists\".\n\nTitles and Labels:\n\nThe labs() function adds titles and labels to the map. The title indicates that the map shows the number of foreign tourists by province and year, with the top 5 provinces highlighted. A caption is added for data sourcing, and the color legend is labeled as “Total Tourists”.\n\nClean Theme:\n\nThe theme_minimal() function is applied to give the plot a clean, minimalistic look.\nFurther customization is made using theme(), where axis text, ticks, grids, and background elements are removed (element_blank()) to maintain a simple and clear presentation.\n\n\n\nggplot(no_tourist_foreign) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = no_tourist_foreign %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"no_tourist_foreign by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019 vs. 2020-2021:\n\n2019 shows the highest concentration of foreign tourists, especially in coastal and central provinces, highlighted in red.\n2020-2021 saw a significant decline, with fewer red areas indicating the top 5 provinces receiving foreign tourists.\n\n2022-2023:\n\nA recovery in foreign tourism begins in 2022 and strengthens in 2023, though still not reaching the 2019 levels.\nThe top provinces remain relatively consistent, focused on key tourist hubs.\n\nNA Data:\n\nA gap year appears with no available data (possibly for 2021), as marked by “NA”.\n\n\n\n\nAnalysis:\n\nCOVID-19’s Impact: Foreign tourism saw a sharp decline in 2020 and 2021, but gradual recovery is evident starting in 2022.\nKey Provinces: Despite the drop, the same provinces (central and coastal) consistently lead in foreign tourism, suggesting strong reliance on specific regions for international tourism recovery."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-tourists-who-stay",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-tourists-who-stay",
    "title": "Take-Home_Ex02",
    "section": "4.3 Plotting a Map with Highlighted Top 5 Provinces by Tourists Who Stay",
    "text": "4.3 Plotting a Map with Highlighted Top 5 Provinces by Tourists Who Stay\n\nBase Map Plot:\n\nThe plot is initiated using the ggplot() function with the dataset no_tourist_stay.\ngeom_sf() is used to map the boundaries of all provinces. The fill aesthetic represents the total_value (number of tourists who stay), while the borders of provinces are outlined in black with a thickness of 0.2.\nfacet_wrap(~ year) divides the map into separate panels by year, allowing you to compare the data across years.\n\nHighlighting Top 5 Provinces:\n\nA second geom_sf() layer is added to highlight the top 5 provinces by the number of tourists who stay in each year. The top_n(5, total_value) function is used to select the top 5 provinces based on the total_value column.\nThese top provinces are highlighted in red (fill = \"red\") with a thicker black border (size = 0.4), making them stand out on the map.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale (plasma option) based on the total number of tourists who stay. The color gradient is named \"Total Tourists\" in the legend, and scale_fill_viridis_c() is used to apply this color scheme.\n\nTitles and Labels:\n\nThe labs() function adds a title, caption, and legend labels. The title indicates that the map shows the number of tourists who stay, by province and year, with the top 5 provinces highlighted.\nThe caption is labeled as “Source: Your Dataset” and the legend is labeled \"Total Tourists\".\n\nClean Theme:\n\nThe theme_minimal() function applies a minimalist theme to the plot.\nAdditional theme customizations (theme()) are used to remove axis text, ticks, grid lines, and background elements to make the plot clean and visually simple.\n\n\n\nggplot(no_tourist_stay) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = no_tourist_stay %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"no_tourist_stay by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019: Highest tourist stay numbers are observed in the northern and southern provinces, highlighted in red.\n2020-2021: Sharp decline in tourist stays due to COVID-19, with fewer provinces (top 5) standing out in red, indicating less overall activity.\n2022-2023: There is a gradual recovery in tourist stays, with some provinces regaining pre-pandemic levels of activity.\n\n\n\nAnalysis:\n\nCOVID-19 Impact: The number of tourist stays significantly decreased in 2020 and 2021, with a gradual recovery seen from 2022 onwards.\nKey Provinces: The same provinces remain the top destinations for longer stays, indicating consistency in popular travel regions, essential for targeting tourism recovery efforts."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-domestic-tourists-thai-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-domestic-tourists-thai-tourists",
    "title": "Take-Home_Ex02",
    "section": "4.4 Plotting a Map with Highlighted Top 5 Provinces by Domestic Tourists (Thai Tourists)",
    "text": "4.4 Plotting a Map with Highlighted Top 5 Provinces by Domestic Tourists (Thai Tourists)\n\nBase Map Plot:\n\nThe ggplot() function initializes the map using the no_tourist_thai dataset, which contains data on the number of Thai tourists by province.\ngeom_sf() is used to plot the boundaries of all provinces. The fill aesthetic is mapped to total_value (the total number of domestic tourists), while the borders of provinces are outlined in black (color = \"black\") with a thickness of 0.2.\nfacet_wrap(~ year) creates separate map panels for each year, allowing for a year-by-year comparison of domestic tourism trends.\n\nHighlighting Top 5 Provinces:\n\nAnother geom_sf() layer is added to highlight the top 5 provinces with the highest number of domestic tourists. The top_n(5, total_value) function is used to select the top 5 provinces based on the total_value column.\nThese top 5 provinces are filled in red (fill = \"red\") with a thicker black border (size = 0.4) to visually emphasize them.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale with the \"plasma\" option, based on the total number of domestic tourists. The color scale is applied using scale_fill_viridis_c(), and the legend is labeled \"Total Tourists\".\n\nTitles and Labels:\n\nThe labs() function adds a title, caption, and legend labels. The title specifies that the map shows the number of Thai tourists by province and year, with the top 5 provinces highlighted. The caption indicates the data source as “Your Dataset”, and the legend is labeled \"Total Tourists\".\n\n\n\nggplot(no_tourist_thai) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = no_tourist_thai %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"no_tourist_thai by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019: The highest number of domestic tourists is concentrated in central and northern provinces, with the top 5 provinces highlighted in red.\n2020-2021: There is a clear decline in domestic tourist numbers, but some provinces still remain in the top 5 throughout the pandemic.\n2022-2023: A recovery is observed, with domestic tourism activity picking up across more provinces, although it hasn’t fully returned to 2019 levels.\n\n\n\nAnalysis:\n\nCOVID-19 Impact: The drop in domestic tourism is less drastic than foreign tourism, suggesting local travel may have been more resilient during the pandemic.\nKey Provinces: The same top provinces remain consistent, indicating they are popular domestic tourist destinations. This consistency provides opportunities for targeted tourism marketing and infrastructure development in these regions."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-ratio-of-tourist-stays",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-ratio-of-tourist-stays",
    "title": "Take-Home_Ex02",
    "section": "4.5 Plotting a Map with Highlighted Top 5 Provinces by Ratio of Tourist Stays",
    "text": "4.5 Plotting a Map with Highlighted Top 5 Provinces by Ratio of Tourist Stays\n\nBase Map Plot:\n\nThe plot is initiated using the ggplot() function with the dataset ratio_tourist_stay.\ngeom_sf() is used to plot the geographical boundaries of all provinces. The fill aesthetic is mapped to the avg_value (average ratio of tourist stays), and the borders are outlined in black with a thickness of 0.2.\nfacet_wrap(~ year) creates separate map panels for each year, allowing for easy comparison of tourist stay ratios over time.\n\nHighlighting Top 5 Provinces:\n\nAnother geom_sf() layer is added to highlight the top 5 provinces with the highest average ratio of tourist stays in each year. The top_n(5, avg_value) function is used to select the top 5 provinces based on the avg_value column.\nThese provinces are filled in red with a thicker black border (size = 0.4), ensuring that they stand out on the map.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale, with the \"plasma\" option for a visually appealing gradient. The color scale is applied using scale_fill_viridis_c(), and the legend is labeled \"Total Tourists\" to represent the average ratio of tourist stays.\n\nTitles and Labels:\n\nThe labs() function adds a title, caption, and labels. The title indicates that the map shows the average ratio of tourist stays by province and year, with the top 5 provinces highlighted. The caption provides the source of the data, and the fill scale is labeled as \"Total Tourists\".\n\nClean Theme:\n\nA minimalistic theme is applied using theme_minimal() to simplify the visual presentation.\nFurther theme customizations are made using theme(), where axis text, ticks, grid lines, and background elements are removed (element_blank()), giving the plot a clean and professional look.\n\n\n\nggplot(ratio_tourist_stay) +\n  geom_sf(aes(fill = avg_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = ratio_tourist_stay %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, avg_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"ratio_tourist_stay by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019-2021: The ratio of tourists staying overnight in provinces fluctuates, with key provinces in the north and central regions highlighted in red. The overall ratio appears to decline during 2021 due to pandemic impacts.\n2022-2023: The ratio begins to recover, with more areas returning to pre-pandemic levels. The provinces highlighted in red indicate the top 5 with the highest ratios of tourists staying overnight.\nNA Data: One year shows missing data for this analysis.\n\n\n\nAnalysis:\n\nTourist Stay Patterns: Despite the impact of COVID-19, certain provinces consistently attract tourists for longer stays, which may reflect their popularity or resilience in tourism infrastructure.\nRecovery: The ratio of overnight stays shows signs of recovery post-pandemic, indicating a return of tourism activity, especially in key tourist destinations."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-total-tourism-revenue",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-total-tourism-revenue",
    "title": "Take-Home_Ex02",
    "section": "4.6 Plotting a Map with Highlighted Top 5 Provinces by Total Tourism Revenue",
    "text": "4.6 Plotting a Map with Highlighted Top 5 Provinces by Total Tourism Revenue\n\nBase Map Plot:\n\nThe ggplot() function is initialized using the revenue_all dataset, which contains data on the total tourism revenue by province.\ngeom_sf() is used to plot the boundaries of all provinces. The fill aesthetic is mapped to total_value (total revenue), and the provinces are outlined in black with a border size of 0.2.\nfacet_wrap(~ year) divides the map into separate panels for each year, making it easier to compare how tourism revenue has changed over time.\n\nHighlighting Top 5 Provinces:\n\nA second geom_sf() layer is added to highlight the top 5 provinces with the highest total tourism revenue in each year. The top_n(5, total_value) function selects the top 5 provinces based on the total_value column.\nThese top provinces are filled in red with a thicker black border (size = 0.4), making them stand out in the map.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale (plasma option) based on the total tourism revenue. The color scale is applied using scale_fill_viridis_c(), and the legend is labeled \"Total Tourists\" to represent the total revenue.\n\nTitles and Labels:\n\nThe labs() function is used to add a title, caption, and legend labels. The title indicates that the map shows total tourism revenue by province and year, with the top 5 provinces highlighted. The caption provides the source of the data as \"Your Dataset\", and the fill scale is labeled as \"Total Tourists\".\n\nClean Theme:\n\nA minimalistic theme is applied using theme_minimal() to ensure a clean presentation.\nAdditional theme customizations are made using theme(), where axis text, ticks, grid lines, and background are removed (element_blank()), giving the plot a clean and professional look.\n\n\n\nggplot(revenue_all) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = revenue_all %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"revenue_all by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019-2021: The highest revenue from tourists is concentrated in a few provinces, highlighted in red, especially in 2019. The revenue drops noticeably in 2020-2021 due to the pandemic, with fewer provinces showing high tourist revenue.\n2022-2023: Recovery of tourist revenue is observed, though not yet at pre-pandemic levels. Key provinces still dominate the revenue generation.\nNA Data: One year has missing data, indicated as “NA.”\n\n\n\nAnalysis:\n\nImpact on Revenue: The reduction in tourist numbers during 2020-2021 significantly affected revenue, as indicated by fewer high-revenue provinces.\nKey Provinces: The same provinces remain critical in generating high tourist revenue across the years, showing their importance in Thailand’s tourism-driven economy and the need for focused recovery strategies."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-foreign-tourism-revenue",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-foreign-tourism-revenue",
    "title": "Take-Home_Ex02",
    "section": "4.7 Plotting a Map with Highlighted Top 5 Provinces by Foreign Tourism Revenue",
    "text": "4.7 Plotting a Map with Highlighted Top 5 Provinces by Foreign Tourism Revenue\n\nBase Map Plot:\n\nThe plot is initiated using ggplot() with the revenue_foreign dataset, which contains data on foreign tourism revenue by province.\ngeom_sf() is used to plot the boundaries of all provinces, where the fill aesthetic is mapped to total_value (total foreign tourism revenue). The borders of the provinces are outlined in black with a thickness of 0.2.\nfacet_wrap(~ year) divides the map into separate panels for each year, enabling year-to-year comparisons of foreign tourism revenue.\n\nHighlighting Top 5 Provinces:\n\nAnother geom_sf() layer is added to highlight the top 5 provinces with the highest foreign tourism revenue in each year. The top_n(5, total_value) function selects the top 5 provinces based on the total_value column.\nThese top provinces are highlighted in red (fill = \"red\") with a thicker black border (size = 0.4) to make them visually stand out.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale with the \"plasma\" option, based on the total foreign tourism revenue. The color gradient is applied using scale_fill_viridis_c(), and the legend is labeled \"Total Tourists\" to represent the revenue scale.\n\nTitles and Labels:\n\nThe labs() function adds a title, caption, and legend labels. The title indicates that the map shows foreign tourism revenue by province and year, with the top 5 provinces highlighted. The caption provides the source of the data as \"Your Dataset\", and the fill scale is labeled \"Total Tourists\".\n\nClean Theme:\n\nA minimalistic theme is applied using theme_minimal() for a clean, simple visual appearance.\nAdditional theme customizations (theme()) remove axis text, ticks, grid lines, and the background (element_blank()), providing a clear and professional presentation.\n\n\n\nggplot(revenue_foreign) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = revenue_foreign %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"revenue_foreign by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019-2021: Foreign tourist revenue is concentrated in key provinces, especially in southern coastal regions and the capital area, as highlighted in red. There is a clear decline in revenue during 2020 and 2021 due to the pandemic, with fewer provinces showing high foreign tourist revenue.\n2022-2023: The data shows a recovery in foreign tourist revenue, with similar key provinces returning as top earners, though the revenue levels have not yet returned to 2019 values.\nNA Data: One year is marked as “NA,” indicating missing data.\n\n\n\nAnalysis:\n\nPandemic Impact: The sharp decline in revenue from foreign tourists during 2020 and 2021 reflects the global restrictions on travel during the COVID-19 pandemic. These maps highlight the provinces most affected by the revenue drop.\nRecovery: From 2022 onward, foreign tourist revenue shows signs of recovery, particularly in the major tourist provinces. These regions should be a focus for post-pandemic recovery strategies to boost foreign tourism and associated revenue."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-domestic-tourism-revenue-thai-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#plotting-a-map-with-highlighted-top-5-provinces-by-domestic-tourism-revenue-thai-tourists",
    "title": "Take-Home_Ex02",
    "section": "4.8 Plotting a Map with Highlighted Top 5 Provinces by Domestic Tourism Revenue (Thai Tourists)",
    "text": "4.8 Plotting a Map with Highlighted Top 5 Provinces by Domestic Tourism Revenue (Thai Tourists)\n\nBase Map Plot:\n\nThe plot is initialized with ggplot() using the revenue_thai dataset, which contains data on the total tourism revenue generated by Thai tourists (domestic tourism) by province.\ngeom_sf() is used to plot the boundaries of all provinces, with the fill aesthetic representing the total_value (total revenue from Thai tourists). The provinces are outlined in black with a border size of 0.2.\nfacet_wrap(~ year) creates separate panels for each year, allowing for year-by-year comparisons of domestic tourism revenue.\n\nHighlighting Top 5 Provinces:\n\nAnother geom_sf() layer is added to highlight the top 5 provinces with the highest domestic tourism revenue in each year. The top_n(5, total_value) function selects the top 5 provinces based on the total_value column.\nThese top provinces are highlighted in red (fill = \"red\") with a thicker black border (size = 0.4), making them stand out.\n\nColor Scale for Other Provinces:\n\nThe remaining provinces are colored using the viridis color scale with the \"plasma\" option. This color gradient reflects the total revenue from domestic tourism across provinces. The color scale is applied using scale_fill_viridis_c(), and the legend is labeled \"Total Tourists\" to represent the revenue scale.\n\nTitles and Labels:\n\nThe labs() function adds titles and labels. The title indicates that the map shows revenue from domestic tourism by province and year, with the top 5 provinces highlighted. The caption provides the data source as \"Your Dataset\", and the fill legend is labeled \"Total Tourists\".\n\nClean Theme:\n\nA minimal theme is applied using theme_minimal() to give the map a clean and simple aesthetic.\nAdditional theme customizations remove axis text, ticks, grid lines, and the background using element_blank(), ensuring a neat and focused visual presentation.\n\n\n\nggplot(revenue_thai) +\n  geom_sf(aes(fill = total_value), color = \"black\", size = 0.2) +\n  facet_wrap(~ year) +\n  geom_sf(data = revenue_thai %&gt;%\n            group_by(year) %&gt;%\n            top_n(5, total_value), \n          aes(geometry = geometry), \n          fill = \"red\", color = \"black\", size = 0.4) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Total Tourists\") +\n  labs(title = \"revenue_thai by Province and Year (Top 5 Highlighted)\",\n       caption = \"Source: Your Dataset\",\n       fill = \"Total Tourists\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_blank())\n\n\n\n\n\n\n\n\n\nObservations:\n\n2019-2021: Domestic tourist revenue is consistently highest in specific provinces (highlighted in red), with the peak in 2019. During 2020 and 2021, there is a decline in revenue across most provinces due to the pandemic, although some regions remain top earners.\n2022-2023: There is a visible recovery in revenue from domestic tourism starting in 2022, with 2023 continuing the trend, although revenue levels have not fully reached pre-pandemic values.\nNA Data: One year shows missing data (NA).\n\n\n\nAnalysis:\n\nPandemic Effects: Domestic tourism revenue fell in 2020-2021, though less drastically compared to foreign tourism, indicating that local tourism provided some resilience to the economy during the pandemic.\nConsistent Key Provinces: Provinces that were top earners before the pandemic remain crucial for domestic tourism, which suggests these areas should be prioritized in strategies to boost local tourism revenue recovery."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-total-tourist-values",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-total-tourist-values",
    "title": "Take-Home_Ex02",
    "section": "5.1 Calculating Global Moran’s I for Spatial Autocorrelation of Total Tourist Values",
    "text": "5.1 Calculating Global Moran’s I for Spatial Autocorrelation of Total Tourist Values\n\nData Cleaning:\n\nThe data is filtered to retain only rows with valid (finite) total_value data. This ensures that the analysis only includes complete and meaningful observations related to total tourist values.\n\nConverting Data to Spatial Format:\n\nThe cleaned data is converted into a spatial data format using the sf (simple features) package, which is commonly used in R for handling spatial data.\n\nConverting Geometries to Centroids:\n\nSince the geometries in the dataset are likely MULTIPOLYGONS, they are simplified by converting them into centroid points. Each polygon (such as a province) is represented by its center point to facilitate spatial analysis.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA spatial weights matrix is constructed using a k-nearest neighbors (k-NN) approach. This matrix is based on the distances between the centroid points, with the parameter k = 5 indicating that each point is linked to its five nearest neighbors.\n\nCalculating Spatial Weights:\n\nThe spatial weights matrix is calculated from the k-NN distances. These weights represent the strength of the spatial relationships between each region (province), based on proximity.\n\nGlobal Moran’s I Calculation:\n\nMoran’s I statistic is computed to assess whether regions with similar tourist values are spatially clustered. A positive Moran’s I indicates a clustering of similar values (e.g., provinces with high tourist numbers are near each other), while a negative value indicates spatial dispersion (e.g., high and low tourist numbers alternate).\n\nPrinting the Result:\n\nFinally, the result of the Moran’s I calculation is printed, which quantifies the degree of spatial autocorrelation in the total_value variable.\n\n\n\ndata_clean &lt;- no_tourist_all %&gt;%\n  filter(is.finite(total_value))\n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.4268961\n\n$K\n[1] 114.7938\n\n\n\nObservations:\n\nMoran’s I Value (0.4268961): The positive value indicates that there is a moderate level of spatial autocorrelation in the data. This suggests that provinces with high or low tourist counts are spatially clustered, meaning that nearby provinces tend to have similar tourist numbers.\nZ-Score ($K = 114.7938$): This Z-score is extremely high, indicating a statistically significant spatial pattern. The tourist data is not randomly distributed but shows a strong spatial dependency.\n\n\n\nAnalysis:\n\nSpatial Clustering: Provinces with high tourist numbers tend to be clustered together, and the same is true for provinces with low tourist numbers. This could be due to regional tourist attractions and infrastructure being concentrated in specific areas.\nTourism Strategy: For policymakers, this spatial autocorrelation suggests that efforts to boost tourism should focus on regions rather than individual provinces, as improvements in one province could have a positive effect on neighboring areas."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-foreign-tourist-values",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-foreign-tourist-values",
    "title": "Take-Home_Ex02",
    "section": "5.2 Calculating Global Moran’s I for Spatial Autocorrelation of Foreign Tourist Values",
    "text": "5.2 Calculating Global Moran’s I for Spatial Autocorrelation of Foreign Tourist Values\n\nData Cleaning:\n\nThe dataset no_tourist_foreign is filtered to retain only rows with valid (finite) total_value entries. This ensures that only complete and meaningful data is included in the analysis, which focuses on foreign tourist numbers.\n\nConverting Data to an sf Object:\n\nThe cleaned dataset is then converted into a simple features (sf) object. This conversion allows the data to be treated as spatial data, necessary for the spatial analysis that follows.\n\nConverting Geometries to Centroids:\n\nSince the original geometries are likely MULTIPOLYGONS, they are converted into centroid points using st_centroid(). Each polygon is represented by a single point, its centroid, simplifying spatial calculations.\n\nCreating a k-Nearest Neighbors Spatial Weights Matrix:\n\nA spatial weights matrix is constructed using the k-nearest neighbors (k-NN) approach. Here, k = 5 specifies that each centroid is connected to its five nearest neighboring centroids. This matrix defines the spatial relationships between provinces based on their proximity.\n\nCalculating Spatial Weights:\n\nUsing the k-NN matrix, spatial weights are calculated. These weights represent the strength of spatial relationships based on the distance between neighboring centroids.\n\nCalculating Global Moran’s I:\n\nMoran’s I is calculated using the global_moran() function, with the foreign tourist total_value as the response variable. The Moran’s I statistic indicates whether there is a spatial autocorrelation in foreign tourist activity. A positive value suggests clustering, while a negative value suggests spatial dispersion.\n\nPrinting the Result:\n\nThe result of the Moran’s I calculation is printed to assess the degree of spatial correlation for foreign tourist data across regions.\n\n\n\ndata_clean &lt;- no_tourist_foreign %&gt;%\n  filter(is.finite(total_value))\n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.3182537\n\n$K\n[1] 155.355\n\n\n\nObservations:\n\nMoran’s I Value (0.3182537): This positive value indicates that there is spatial autocorrelation in the foreign tourist data, but it is weaker than the previous dataset (0.4269). This suggests that there is still a tendency for provinces with similar foreign tourist counts to cluster together, but the clustering is not as strong as for overall tourists.\nZ-Score ($K = 155.355$): This Z-score is even higher than the previous analysis, indicating a very strong statistical significance. The spatial clustering pattern is highly non-random.\n\n\n\nAnalysis:\n\nSpatial Clustering of Foreign Tourists: There is some clustering of foreign tourist activity in specific regions, but it is less pronounced than the overall tourist data. This might indicate that while certain regions attract both domestic and foreign tourists, foreign tourists are slightly more dispersed across the provinces.\nStrategic Insights: Since foreign tourists are somewhat clustered but more spatially distributed compared to overall tourists, tourism development strategies for attracting foreign visitors could benefit from diversifying and expanding into regions outside of the traditional tourist hubs."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-tourist-stays",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-tourist-stays",
    "title": "Take-Home_Ex02",
    "section": "5.3 Calculating Global Moran’s I for Spatial Autocorrelation of Tourist Stays",
    "text": "5.3 Calculating Global Moran’s I for Spatial Autocorrelation of Tourist Stays\n\nData Cleaning:\n\nThe data is filtered to retain only rows where the total_value (number of tourists who stay) is finite. This ensures that only valid data points are included in the subsequent spatial analysis.\n\nConverting Data to sf Object:\n\nThe cleaned dataset is converted into an sf (simple features) object, enabling the application of spatial operations and analysis.\n\nConverting Geometries to Centroids:\n\nSince the dataset contains MULTIPOLYGON geometries, each province is represented by its centroid (a single point), using the st_centroid() function. This simplifies the spatial representation and is more suitable for distance-based spatial analysis.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA k-nearest neighbors (k-NN) spatial weights matrix is created, where k = 5 indicates that each province (represented by its centroid) is linked to its five nearest neighbors. This matrix defines the spatial relationships between provinces based on proximity.\n\nCalculating Spatial Weights:\n\nUsing the spatial weights matrix (nb_dist), spatial weights are computed, which quantify the strength of the spatial relationships between neighboring provinces.\n\nCalculating Global Moran’s I:\n\nThe global_moran() function is used to calculate Moran’s I for the total_value (number of tourists staying) variable. Moran’s I measures the spatial autocorrelation: a positive value suggests clustering (provinces with similar numbers of tourist stays are near each other), while a negative value indicates dispersion (provinces with dissimilar numbers of tourist stays are near each other).\n\nPrinting the Result:\n\nThe final result, the Moran’s I statistic, is printed. This statistic provides insight into whether spatial patterns exist in the distribution of tourist stays across provinces.\n\n\n\ndata_clean &lt;- no_tourist_stay %&gt;%\n  filter(is.finite(total_value))\n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.3636701\n\n$K\n[1] 115.8772\n\n\n\nObservations:\n\nMoran’s I Value (0.3636701): The positive value suggests that there is a moderate level of spatial autocorrelation in the number of tourist stays. This implies that provinces with similar numbers of tourist stays tend to be spatially clustered.\nZ-Score ($K = 115.8772$): This Z-score is quite high, indicating strong statistical significance. This confirms that the spatial pattern of tourist stays is non-random and that nearby provinces exhibit similar tourist stay behaviors.\n\n\n\nAnalysis:\n\nSpatial Clustering: The moderate Moran’s I value suggests that provinces with high or low numbers of tourist stays are clustered. These clusters could be indicative of key tourism regions that are popular for overnight stays, likely due to their infrastructure or attractions.\nTourism Strategy: Since tourist stays are clustered, expanding tourism infrastructure or marketing to nearby provinces may capitalize on existing tourist flows. Provinces that neighbor high-stay regions could attract more visitors by improving accommodations or promoting longer stays."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-domestic-tourists-thai-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-domestic-tourists-thai-tourists",
    "title": "Take-Home_Ex02",
    "section": "5.4 Calculating Global Moran’s I for Spatial Autocorrelation of Domestic Tourists (Thai Tourists)",
    "text": "5.4 Calculating Global Moran’s I for Spatial Autocorrelation of Domestic Tourists (Thai Tourists)\n\nData Cleaning:\n\nThe dataset no_tourist_thai is filtered to retain only rows where the total_value (number of Thai tourists) is finite. This step ensures that invalid or missing data is removed before proceeding with the spatial analysis.\n\nConverting Data to sf Object:\n\nThe cleaned dataset is converted into a simple features (sf) object using the st_as_sf() function. This conversion is necessary for conducting spatial analysis, as it treats the data as geographical information.\n\nConverting Geometries to Centroids:\n\nSince the original geometries are MULTIPOLYGONS (e.g., province boundaries), they are simplified by converting each polygon to its centroid using st_centroid(). This transformation reduces the geometries to a single point, allowing for more straightforward distance-based spatial analysis.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA spatial weights matrix is created using a k-nearest neighbors approach. The matrix is constructed such that each centroid is linked to its five nearest neighbors (k = 5). This defines the spatial relationships between the provinces based on proximity.\n\nCalculating Spatial Weights:\n\nThe spatial weights are calculated from the k-NN matrix using the st_weights() function. These weights represent the strength of spatial relationships between neighboring provinces based on the k-nearest neighbors.\n\nCalculating Global Moran’s I:\n\nMoran’s I is computed using the global_moran() function. The total_value variable, representing the number of Thai tourists, is passed to the function along with the spatial weights and the k-NN matrix. Moran’s I provides a measure of spatial autocorrelation: positive values indicate clustering (provinces with similar numbers of tourists are near each other), while negative values suggest dispersion (provinces with dissimilar numbers of tourists are near each other).\n\nPrinting the Result:\n\nFinally, the result of the Moran’s I calculation is printed, allowing for the interpretation of spatial autocorrelation in domestic tourism data.\n\n\n\ndata_clean &lt;- no_tourist_thai %&gt;%\n  filter(is.finite(total_value))\n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.4700614\n\n$K\n[1] 76.83119\n\n\n\nObservations:\n\nMoran’s I Value (0.4700614): The positive value of Moran’s I indicates a moderate to high level of spatial autocorrelation. This means that provinces with similar Thai tourist numbers (high or low) tend to cluster spatially, showing stronger spatial dependence than other datasets you’ve analyzed.\nZ-Score ($K = 76.83119$): The Z-score is lower compared to previous analyses, but it still shows strong statistical significance, confirming that the observed spatial pattern of Thai tourist distribution is not random.\n\n\n\nAnalysis:\n\nSpatial Clustering of Thai Tourists: The stronger spatial autocorrelation for domestic (Thai) tourists suggests that provinces with similar tourist numbers are more likely to be geographically close. This could be due to regional preferences for domestic travel, as tourists may prefer certain areas for proximity or cultural familiarity.\nTourism Strategy: The strong clustering suggests that expanding tourism development in provinces adjacent to high-tourism areas may encourage domestic tourists to explore neighboring regions. Focusing efforts on less-visited provinces near popular tourist hubs could help distribute the tourism flow more evenly across the country."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-total-tourism-revenue",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-total-tourism-revenue",
    "title": "Take-Home_Ex02",
    "section": "5.5 Calculating Global Moran’s I for Spatial Autocorrelation of Total Tourism Revenue",
    "text": "5.5 Calculating Global Moran’s I for Spatial Autocorrelation of Total Tourism Revenue\n\nData Cleaning:\n\nThe dataset revenue_all is filtered to retain only rows where the total_value (representing total tourism revenue) is finite. This ensures that only valid data points are used for the spatial analysis.\n\nConverting Data to an sf Object:\n\nThe cleaned data is then converted into a simple features (sf) object using st_as_sf(). This transformation enables spatial operations, as the data is now treated as geographical information.\n\nConverting Geometries to Centroids:\n\nThe original MULTIPOLYGON geometries (e.g., provincial boundaries) are converted into centroid points using st_centroid(). This conversion simplifies the spatial representation by reducing each polygon to a single representative point.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA k-nearest neighbors (k-NN) spatial weights matrix is constructed, with each centroid linked to its five nearest neighbors (k = 5). This matrix establishes spatial relationships between neighboring provinces.\n\nCalculating Spatial Weights:\n\nSpatial weights are calculated based on the k-NN matrix using the st_weights() function. These weights quantify the spatial relationships between neighboring provinces.\n\nCalculating Global Moran’s I:\n\nMoran’s I is computed using the global_moran() function. The total_value variable, representing the total tourism revenue, is passed to the function along with the spatial weights and k-NN matrix. Moran’s I indicates spatial autocorrelation: a positive value suggests clustering (provinces with similar revenue are near each other), while a negative value indicates spatial dispersion (provinces with differing revenue are near each other).\n\nPrinting the Result:\n\nThe result of Moran’s I calculation is printed, providing a measure of the spatial autocorrelation in the total tourism revenue data.\n\n\n\ndata_clean &lt;- revenue_all %&gt;%\n  filter(is.finite(total_value)) \n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.2947111\n\n$K\n[1] 174.8506\n\n\n\nObservations:\n\nMoran’s I Value (0.2947111): This value suggests a positive but relatively weak level of spatial autocorrelation. While there is some clustering of similar revenue values (high or low), it is less pronounced than in other datasets.\nZ-Score ($K = 174.8506$): The Z-score is extremely high, confirming that the observed spatial pattern is statistically significant and highly unlikely to be random.\n\n\n\nAnalysis:\n\nWeaker Clustering of Tourism Revenue: The weaker Moran’s I value implies that tourism revenue is more dispersed across provinces, meaning that high-revenue provinces may not be as spatially clustered as tourist numbers. This could reflect varying levels of spending by tourists across different regions.\nTargeting High Revenue Areas: Although there is less spatial clustering, certain regions still exhibit strong tourist spending patterns. Tourism strategies could focus on understanding why some high-tourist-number regions generate lower revenues and look for ways to increase tourist spending in those areas."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-foreign-tourism-revenue",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-foreign-tourism-revenue",
    "title": "Take-Home_Ex02",
    "section": "5.6 Calculating Global Moran’s I for Spatial Autocorrelation of Foreign Tourism Revenue",
    "text": "5.6 Calculating Global Moran’s I for Spatial Autocorrelation of Foreign Tourism Revenue\n\nData Cleaning:\n\nThe dataset revenue_foreign is filtered to retain only rows with finite (total_value), ensuring that only valid observations of foreign tourism revenue are included in the analysis.\n\nConverting Data to sf Object:\n\nThe cleaned dataset is converted into an sf (simple features) object using st_as_sf(). This transformation enables spatial analysis, as the data is treated as geographical information.\n\nConverting Geometries to Centroids:\n\nSince the original geometries are MULTIPOLYGONS (e.g., provincial boundaries), they are converted into centroids using st_centroid(). This conversion simplifies the spatial data by representing each polygon as a single point.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA k-nearest neighbors spatial weights matrix is created using the st_knn() function, where each centroid is linked to its five nearest neighbors (k = 5). This matrix defines the spatial relationships between provinces based on proximity.\n\nCalculating Spatial Weights:\n\nThe spatial weights are calculated from the k-NN matrix using the st_weights() function. These weights quantify the spatial relationships between neighboring provinces.\n\nCalculating Global Moran’s I:\n\nMoran’s I is calculated using the global_moran() function, with the total_value variable (foreign tourism revenue) passed to the function along with the spatial weights and the k-NN matrix. Moran’s I indicates the degree of spatial autocorrelation: a positive value suggests clustering (provinces with similar foreign tourism revenue are near each other), while a negative value indicates dispersion (provinces with differing revenue are near each other).\n\nPrinting the Result:\n\nThe result of the Moran’s I calculation is printed, providing insight into the spatial autocorrelation of foreign tourism revenue.\n\n\n\ndata_clean &lt;- revenue_foreign %&gt;%\n  filter(is.finite(total_value)) \n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.27495\n\n$K\n[1] 152.6115\n\n\n\nObservations:\n\nMoran’s I Value (0.27495): This value indicates a weak positive spatial autocorrelation. It suggests that provinces with similar foreign tourist revenues tend to cluster, but the clustering is not very strong.\nZ-Score ($K = 152.6115$): Despite the weak Moran’s I value, the Z-score is high, indicating that the observed spatial pattern is statistically significant and not random.\n\n\n\nAnalysis:\n\nWeak Clustering of Foreign Tourist Revenue: The weak Moran’s I suggests that foreign tourist revenue is more spatially dispersed, meaning high-revenue provinces are not strongly concentrated in specific regions. Foreign tourists may be spending more evenly across different provinces, or revenue patterns could be influenced by factors such as tourist length of stay, spending habits, or regional tourist attractions.\nStrategic Focus: Although clustering is weak, it still exists. This means that certain regions attract higher foreign tourist spending and could be prioritized for infrastructure improvements or marketing campaigns to boost revenue further. Additionally, understanding why some regions with high tourist numbers generate less revenue could reveal areas for development."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-domestic-tourism-revenue-thai-tourists",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex02.html#calculating-global-morans-i-for-spatial-autocorrelation-of-domestic-tourism-revenue-thai-tourists",
    "title": "Take-Home_Ex02",
    "section": "5.7 Calculating Global Moran’s I for Spatial Autocorrelation of Domestic Tourism Revenue (Thai Tourists)",
    "text": "5.7 Calculating Global Moran’s I for Spatial Autocorrelation of Domestic Tourism Revenue (Thai Tourists)\n\nData Cleaning:\n\nThe dataset revenue_thai is filtered to retain only rows with valid (finite) total_value, which represents domestic tourism revenue. This ensures that only complete data points are included in the analysis.\n\nConverting Data to an sf Object:\n\nThe cleaned data is converted into an sf (simple features) object using st_as_sf(). This transformation allows the data to be used for spatial analysis.\n\nConverting Geometries to Centroids:\n\nThe geometries in the dataset are likely MULTIPOLYGONS, representing provincial boundaries. These are simplified by converting them to centroid points using st_centroid(), which reduces each polygon to a single representative point.\n\nCreating a k-Nearest Neighbors (k-NN) Spatial Weights Matrix:\n\nA k-nearest neighbors spatial weights matrix is created using st_knn(), where each centroid is linked to its five nearest neighbors (k = 5). This matrix defines the spatial relationships between provinces based on proximity.\n\nCalculating Spatial Weights:\n\nThe spatial weights are calculated from the k-NN matrix using st_weights(). These weights quantify the spatial relationships between neighboring provinces based on distance.\n\nCalculating Global Moran’s I:\n\nMoran’s I is calculated using the global_moran() function, with the total_value (domestic tourism revenue) passed as the response variable. The spatial weights and k-NN matrix are also included. Moran’s I provides insight into the degree of spatial autocorrelation: a positive value indicates clustering (provinces with similar revenue are near each other), while a negative value indicates spatial dispersion (provinces with dissimilar revenue are near each other).\n\nPrinting the Result:\n\nThe result of the Moran’s I calculation is printed, showing whether there is spatial clustering or dispersion in domestic tourism revenue across provinces.\n\n\n\ndata_clean &lt;- revenue_thai %&gt;%\n  filter(is.finite(total_value)) \n\n\ndata_sf &lt;- st_as_sf(data_clean)\n\ndata_sf_centroid &lt;- data_sf %&gt;% mutate(geometry = st_centroid(geometry))\n\nnb_dist &lt;- st_knn(data_sf_centroid, k = 5)\n\nwt_dist &lt;- st_weights(nb_dist)\n\nmoran &lt;- global_moran(x = data_sf_centroid$total_value, nb = nb_dist, wt = wt_dist)\n\nprint(moran)\n\n$I\n[1] 0.3428837\n\n$K\n[1] 175.4336\n\n\n\nObservations:\n\nMoran’s I Value (0.3428837): This value suggests a moderate level of positive spatial autocorrelation. Provinces with similar levels of Thai tourist revenue are moderately clustered, indicating that areas with high or low revenue tend to be geographically close.\nZ-Score ($K = 175.4336$): The high Z-score confirms that the observed spatial pattern is statistically significant and unlikely to be random.\n\n\n\nAnalysis:\n\nModerate Clustering of Thai Tourist Revenue: The moderate Moran’s I indicates that provinces generating similar levels of revenue from domestic tourists tend to cluster. This could be due to cultural or geographic factors that make certain regions more appealing to Thai tourists, or it may reflect infrastructure and accessibility that influence spending patterns.\nRegional Focus for Domestic Tourism: Since there is moderate clustering, regional tourism strategies can focus on enhancing the experience in neighboring provinces of high-revenue areas to encourage greater domestic tourism. Understanding which regions drive the most revenue from Thai tourists and why can help distribute tourism benefits more evenly."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/OldFile.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/OldFile.html",
    "title": "Take-Home_Exercise 1",
    "section": "",
    "text": "pacman::p_load(sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate)\n\n\n#! eval:false\nacc &lt;- read_csv(\"data/rawdata/thai_road_accident_2019_2022.csv\") %&gt;%\n  mutate(Month_num = month(incident_datetime)) %&gt;%\n  mutate(Month_fac = month(incident_datetime,\n                       label = TRUE,\n                       abbr = TRUE)) %&gt;%\n  mutate(dayofweek = weekdays(incident_datetime))%&gt;%\n  filter(!is.na(longitude) & !is.na(latitude)) %&gt;%  # Remove rows with missing coordinates\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 32647)\n\n\nbangkok_acc &lt;- acc %&gt;%\n  filter(province_en == \"Bangkok\")\n\n\nwrite_rds(bangkok_acc, \"data/rds/bangkok_acc_data.rds\")\n\n\nbangkok_acc_data &lt;- read_rds(\"data/rds/bangkok_acc_data.rds\")\n\n\nthai_one_map &lt;- st_read(dsn = \"data/rawdata\",\n                        layer = \"hotosm_tha_roads_lines_shp\")\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA\n\n\n\nadmin_boundaries &lt;- st_read(dsn = \"data/rawdata\",\n                        layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\n\nbangkok_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Bangkok\")\n\n\nst_crs(thai_one_map) &lt;- 4326\n\n\nbangkok_roads &lt;- st_intersection(thai_one_map, bangkok_boundary)\n\n\nwrite_rds(bangkok_roads, \"data/rds/bangkok_roads.rds\")\n\n\nbangkok_roads_data &lt;- read_rds(\"data/rds/bangkok_roads.rds\")\n\n\nggplot() +\n  geom_sf(data = bangkok_roads_data, color = \"gray\") + \n  geom_sf(data = bangkok_acc_data, aes(color = accident_type), alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In Class Exercise 07",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)\n\npackage 'glue' successfully unpacked and MD5 sums checked\npackage 'rlang' successfully unpacked and MD5 sums checked\npackage 'ggstatsplot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\cttdn\\AppData\\Local\\Temp\\RtmpiG1xtG\\downloaded_packages\npackage 'cli' successfully unpacked and MD5 sums checked\npackage 'glue' successfully unpacked and MD5 sums checked\npackage 'gtsummary' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\cttdn\\AppData\\Local\\Temp\\RtmpiG1xtG\\downloaded_packages"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#ura-master-plan-2014-planning-subzone-boundary",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#ura-master-plan-2014-planning-subzone-boundary",
    "title": "In Class Exercise 07",
    "section": "2.1 URA Master Plan 2014 planning subzone boundary",
    "text": "2.1 URA Master Plan 2014 planning subzone boundary\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\n# mpsz &lt;- read_rds(\"data/geospatial/mpsz.rds\")\n\n\n# condo_resale_sf &lt;- read_rds(\n#   \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#generating-tidy-linear-regression-report",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#generating-tidy-linear-regression-report",
    "title": "In Class Exercise 07",
    "section": "5.1 Generating tidy linear regression report",
    "text": "5.1 Generating tidy linear regression report\n\n# ols_regress(condo_mlr)\n\n\nMulticolinearuty\n\n# ols_vif_tol(condo_mlr)\n\n\n\nVariable selection\n\n# condo_fw_mlr &lt;- ols_step_forward_p(\n#   condo_mlr,\n#   p_val = 0.05,\n#   details = FALSE)\n\n\n# plot(condo_fw_mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-model-parameters",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-model-parameters",
    "title": "In Class Exercise 07",
    "section": "5.2 Visualising model parameters",
    "text": "5.2 Visualising model parameters\n\n# ggcoefstats(condo_mlr,\n#             sort = \"ascending\")\n\n\nTest for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n# ols_plot_resid_fit(condo_fw_mlr$model)\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#test-for-normality-assumption",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#test-for-normality-assumption",
    "title": "In Class Exercise 07",
    "section": "5.3 Test for Normality Assumption",
    "text": "5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n# ols_plot_resid_hist(condo_fw_mlr$model)\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\n# ols_test_normality(condo_fw_mlr$model)\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-stationary-test",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#spatial-stationary-test",
    "title": "In Class Exercise 07",
    "section": "6.1 Spatial stationary test",
    "text": "6.1 Spatial stationary test\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nHo: The residuals are randomly distributed (also known as spatial stationary) H1: The residuals are spatially non-stationary\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\n# condo_resale_sf &lt;- condo_resale_sf %&gt;%\n#   mutate(nb = st_knn(geometry, k=6,\n#                      longlat = FALSE),\n#          wt = st_weights(nb,\n#                          style = \"W\"),\n#          .before = 1)\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\n# global_moran_perm(condo_resale_sf$MLR_RES, \n#                   condo_resale_sf$nb, \n#                   condo_resale_sf$wt, \n#                   alternative = \"two.sided\", \n#                   nsim = 99)\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#building-fixed-bandwidth-gwr-model",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#building-fixed-bandwidth-gwr-model",
    "title": "In Class Exercise 07",
    "section": "7.1 Building Fixed Bandwidth GWR Model",
    "text": "7.1 Building Fixed Bandwidth GWR Model\n\nComputing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach agreement.\n\n# bw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n#                      PROX_CBD + PROX_CHILDCARE + \n#                      PROX_ELDERLYCARE   + PROX_URA_GROWTH_AREA + \n#                      PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n#                      PROX_SHOPPING_MALL + PROX_BUS_STOP + \n#                      NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                    data=condo_resale_sf, \n#                    approach=\"CV\", \n#                    kernel=\"gaussian\", \n#                    adaptive=FALSE, \n#                    longlat=FALSE)\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\nGWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n# gwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n#                          AGE    + PROX_CBD + PROX_CHILDCARE + \n#                          PROX_ELDERLYCARE   +PROX_URA_GROWTH_AREA + \n#                          PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +\n#                          PROX_SHOPPING_MALL + PROX_BUS_STOP + \n#                          NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                        data=condo_resale_sf, \n#                        bw=bw_fixed, \n#                        kernel = 'gaussian', \n#                        longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\n# gwr_fixed\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#building-adaptive-bandwidth-gwr-model",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#building-adaptive-bandwidth-gwr-model",
    "title": "In Class Exercise 07",
    "section": "7.2 Building Adaptive Bandwidth GWR Model",
    "text": "7.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\nComputing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n# bw_adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n#                         PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n#                         PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n#                         PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n#                         NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                       data=condo_resale_sf, \n#                       approach=\"CV\", \n#                       kernel=\"gaussian\", \n#                       adaptive=TRUE, \n#                       longlat=FALSE)\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n# gwr_adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n#                             PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n#                             PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n#                             PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n#                             NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                           data=condo_resale_sf, \n#                           bw=bw_adaptive, \n#                           kernel = 'gaussian', \n#                           adaptive=TRUE, \n#                           longlat = FALSE)\n\nThe code below can be used to display the model output.\n\n# gwr_adaptive\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-gwr-output",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-gwr-output",
    "title": "In Class Exercise 07",
    "section": "7.3 Visualising GWR Output",
    "text": "7.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-sdf-into-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#converting-sdf-into-sf-data.frame",
    "title": "In Class Exercise 07",
    "section": "7.4 Converting SDF into sf data.frame",
    "text": "7.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n# gwr_adaptive_output &lt;- as.data.frame(\n#   gwr_adaptive$SDF) %&gt;%\n#   select(-c(2:15))\n\n\n# gwr_sf_adaptive &lt;- cbind(condo_resale_sf,\n#                          gwr_adaptive_output)\n\nNext, glimpse() is used to display the content of condo_resale_sf.adaptive sf data frame.\n\n# glimpse(gwr_sf_adaptive)\n\n\n# summary(gwr_adaptive$SDF$yhat)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-local-r2",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-local-r2",
    "title": "In Class Exercise 07",
    "section": "7.5 Visualising local R2",
    "text": "7.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\n# tmap_mode(\"view\")\n# tmap_options(check.and.fix = TRUE)\n# tm_shape(mpsz)+\n#   tm_polygons(alpha = 0.1) +\n# tm_shape(gwr_sf_adaptive) +  \n#   tm_dots(col = \"Local_R2\",\n#           border.col = \"gray60\",\n#           border.lwd = 1) +\n#   tm_view(set.zoom.limits = c(11,14))\n\n\n# tmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-coefficient-estimates",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html#visualising-coefficient-estimates",
    "title": "In Class Exercise 07",
    "section": "7.6 Visualising coefficient estimates",
    "text": "7.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\n# tmap_options(check.and.fix = TRUE)\n# tmap_mode(\"view\")\n# AREA_SQM_SE &lt;- tm_shape(mpsz)+\n#   tm_polygons(alpha = 0.1) +\n# tm_shape(gwr_sf_adaptive) +  \n#   tm_dots(col = \"AREA_SQM_SE\",\n#           border.col = \"gray60\",\n#           border.lwd = 1) +\n#   tm_view(set.zoom.limits = c(11,14))\n# \n# AREA_SQM_TV &lt;- tm_shape(mpsz)+\n#   tm_polygons(alpha = 0.1) +\n# tm_shape(gwr_sf_adaptive) +  \n#   tm_dots(col = \"AREA_SQM_TV\",\n#           border.col = \"gray60\",\n#           border.lwd = 1) +\n#   tm_view(set.zoom.limits = c(11,14))\n# \n# tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n#              asp=1, ncol=2,\n#              sync = TRUE)\n\n\n# tmap_mode(\"plot\")\n\n\nBy URA Plannign Region\n\n# tm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n#   tm_polygons()+\n# tm_shape(gwr_sf_adaptive) + \n#   tm_bubbles(col = \"Local_R2\",\n#            size = 0.15,\n#            border.col = \"gray60\",\n#            border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "Introducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#content",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#content",
    "title": "In-class_Ex05",
    "section": "",
    "text": "Introducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "In-class_Ex05",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\nCode\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#the-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#the-data",
    "title": "In-class_Ex05",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-attribute-table",
    "title": "In-class_Ex05",
    "section": "Importing Attribute Table",
    "text": "Importing Attribute Table\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#combining-both-data-frame-by-using-left-join",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#combining-both-data-frame-by-using-left-join",
    "title": "In-class_Ex05",
    "section": "Combining both data frame by using left join",
    "text": "Combining both data frame by using left join\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\nCode\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\nJoining with `by = join_by(County)`\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#plotting-a-choropleth-map",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#plotting-a-choropleth-map",
    "title": "In-class_Ex05",
    "section": "Plotting a choropleth map",
    "text": "Plotting a choropleth map\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#global-measures-of-spatial-association",
    "title": "In-class_Ex05",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\n\nStep 1: Deriving Queen’s contiguity weights: sfdep methods\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\nCode\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n\n\nThe wm_q\n\n\nCode\nwm_q\n\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\nComputing Global Moran’ I\nIn the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.\n\n\nCode\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\nPerforming Global Moran’sI test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.\n\n\nCode\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nThe default for alternative argument is “two.sided”. Other supported arguments are “greater” or “less”. randomization, and\nBy default the randomization argument is TRUE. If FALSE, under the assumption of normality.\n\n\n\nPerforming Global Moran’I permutation test\nIn practice, Monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\nIt is always a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\n\nCode\nset.seed(1234)\n\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\n\nCode\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#lisa-map",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#lisa-map",
    "title": "In-class_Ex05",
    "section": "LISA map",
    "text": "LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-local-morans-i",
    "title": "In-class_Ex05",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\n\nCode\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\nVisualising local Moran’s I\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of GDPPC\",\n    main.title.size = 2)\n\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)\n\n\n\n\n\n\n\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\nVisualising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting LISA map\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\nCode\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class_Ex05",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.\n\n\nComputing local Gi* statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\n\nCode\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wts = st_inverse_distance(nb, \n                              geometry, \n                              scale = 1,\n                              alpha = 1),\n         .before = 1)\n\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\nSince we are going to compute Gi* statistics, include_self()is used.\n\nNow, we will compute the local Gi* by using the code chunk below.\n\n\nCode\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n    gi_star cluster     e_gi  var_gi std_dev p_value p_sim p_folded_sim skewness\n      &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.261   Low     0.00126  1.07e-7  0.283  7.78e-1  0.66         0.33    0.783\n 2 -0.276   Low     0.000969 4.76e-8 -0.123  9.02e-1  0.98         0.49    0.713\n 3  0.00573 High    0.00156  2.53e-7 -0.0571 9.54e-1  0.78         0.39    0.972\n 4  0.528   High    0.00155  2.97e-7  0.321  7.48e-1  0.56         0.28    0.942\n 5  0.466   High    0.00137  2.76e-7  0.386  7.00e-1  0.52         0.26    1.32 \n 6 -0.445   High    0.000992 7.08e-8 -0.588  5.57e-1  0.68         0.34    0.692\n 7  2.99    High    0.000700 4.05e-8  3.13   1.74e-3  0.04         0.02    0.975\n 8  2.04    High    0.00152  1.58e-7  1.77   7.59e-2  0.16         0.08    1.26 \n 9  4.42    High    0.00130  1.18e-7  4.22   2.39e-5  0.02         0.01    1.20 \n10  1.21    Low     0.00175  1.25e-7  1.49   1.36e-1  0.18         0.09    0.408\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\n\nVisualising Gi*\nIn the code chunk below, tmap functions are used to plot the local Gi* (i.e. gi_star) at the province level.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising p-value of HCSA\nIn the code chunk below, tmap functions are used to plot the p-values of local Gi* (i.e. p_sim) at the province level.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nVisuaising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising hot spot and cold spot areas\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\n\nCode\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4)\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class_Ex03",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis\n\n\n\n\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format.\n\n\n\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\n\nCode\npacman::p_load(sf, spNetwork, tmap, tidyverse)\n\n\n\n\n\nThe code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\n\nCode\n# network &lt;- st_read(dsn=\"data/rawdata\", \n#                    layer=\"Punggol_St\")\n\n\n\n\nCode\n# childcare &lt;- st_read(dsn=\"data/rawdata\",\n#                      layer=\"Punggol_CC\")\n\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\n\nCode\n# childcare\n\n\n\n\nCode\n# network\n\n\nWhen I exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information.\n\n\n\nBefore we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\n\nCode\n# plot(st_geometry(network))\n# plot(childcare,add=T,col='red',pch = 19)\n\n\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\n\nCode\n# tmap_mode('view')\n# tm_shape(childcare) + \n#   tm_dots() + \n#   tm_shape(network) +\n#   tm_lines()\n\n\n\n\nCode\n# tmap_mode('plot')\n\n\n\n\n\nIn this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\n\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode\n# lixels &lt;- lixelize_lines(network, \n#                          700, \n#                          mindist = 375)\n\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\n\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode\n# samples &lt;- lines_center(lixels) \n\n\nThe points are located at center of the line based on the length of the line.\n\n\n\nWe are ready to computer the NKDE by using the code chunk below.\n\n\nCode\n# densities &lt;- nkde(network, \n#                   events = childcare,\n#                   w = rep(1, nrow(childcare)),\n#                   samples = samples,\n#                   kernel_name = \"quartic\",\n#                   bw = 300, \n#                   div= \"bw\", \n#                   method = \"simple\", \n#                   digits = 1, \n#                   tol = 1,\n#                   grid_shape = c(1,1), \n#                   max_depth = 8,\n#                   agg = 5, \n#                   sparse = TRUE,\n#                   verbose = FALSE)\n\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\n\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode\n# samples$density &lt;- densities\n# lixels$density &lt;- densities\n\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n\nCode\n# rescaling to help the mapping\n# samples$density &lt;- samples$density*1000\n# lixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode\n# tmap_mode('view')\n# tm_shape(lixels)+\n#   tm_lines(col=\"density\")+\n# tm_shape(childcare)+\n#   tm_dots()\n# tmap_mode('plot')\n\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)\n\n\n\n\n\nIn this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\n\nCode\n# kfun_childcare &lt;- kfunctions(network, \n#                              childcare,\n#                              start = 0, \n#                              end = 1000, \n#                              step = 50, \n#                              width = 50, \n#                              nsim = 50, \n#                              resolution = 50,\n#                              verbose = FALSE, \n#                              conf_int = 0.05)\n\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\n\nCode\n# kfun_childcare$plotk\n\n\n\nThe blue line is the empirical network K-function of the childcare centres in Punggol planning area. The gray envelop represents the results of the 50 simulations in the interval 2.5% - 97.5%. Because the blue line between the distance of 250m-400m are below the gray area, we can infer that the childcare centres in Punggol planning area resemble regular pattern at the distance of 250m-400m."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "title": "In-class_Ex03",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "title": "In-class_Ex03",
    "section": "",
    "text": "In this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "title": "In-class_Ex03",
    "section": "",
    "text": "In this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\n\nCode\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "title": "In-class_Ex03",
    "section": "",
    "text": "The code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\n\nCode\n# network &lt;- st_read(dsn=\"data/rawdata\", \n#                    layer=\"Punggol_St\")\n\n\n\n\nCode\n# childcare &lt;- st_read(dsn=\"data/rawdata\",\n#                      layer=\"Punggol_CC\")\n\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\n\nCode\n# childcare\n\n\n\n\nCode\n# network\n\n\nWhen I exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "title": "In-class_Ex03",
    "section": "",
    "text": "Before we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\n\nCode\n# plot(st_geometry(network))\n# plot(childcare,add=T,col='red',pch = 19)\n\n\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\n\nCode\n# tmap_mode('view')\n# tm_shape(childcare) + \n#   tm_dots() + \n#   tm_shape(network) +\n#   tm_lines()\n\n\n\n\nCode\n# tmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "title": "In-class_Ex03",
    "section": "",
    "text": "In this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\n\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode\n# lixels &lt;- lixelize_lines(network, \n#                          700, \n#                          mindist = 375)\n\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\n\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode\n# samples &lt;- lines_center(lixels) \n\n\nThe points are located at center of the line based on the length of the line.\n\n\n\nWe are ready to computer the NKDE by using the code chunk below.\n\n\nCode\n# densities &lt;- nkde(network, \n#                   events = childcare,\n#                   w = rep(1, nrow(childcare)),\n#                   samples = samples,\n#                   kernel_name = \"quartic\",\n#                   bw = 300, \n#                   div= \"bw\", \n#                   method = \"simple\", \n#                   digits = 1, \n#                   tol = 1,\n#                   grid_shape = c(1,1), \n#                   max_depth = 8,\n#                   agg = 5, \n#                   sparse = TRUE,\n#                   verbose = FALSE)\n\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\n\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode\n# samples$density &lt;- densities\n# lixels$density &lt;- densities\n\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n\nCode\n# rescaling to help the mapping\n# samples$density &lt;- samples$density*1000\n# lixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode\n# tmap_mode('view')\n# tm_shape(lixels)+\n#   tm_lines(col=\"density\")+\n# tm_shape(childcare)+\n#   tm_dots()\n# tmap_mode('plot')\n\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "title": "In-class_Ex03",
    "section": "",
    "text": "In this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\n\nCode\n# kfun_childcare &lt;- kfunctions(network, \n#                              childcare,\n#                              start = 0, \n#                              end = 1000, \n#                              step = 50, \n#                              width = 50, \n#                              nsim = 50, \n#                              resolution = 50,\n#                              verbose = FALSE, \n#                              conf_int = 0.05)\n\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\n\nCode\n# kfun_childcare$plotk\n\n\n\nThe blue line is the empirical network K-function of the childcare centres in Punggol planning area. The gray envelop represents the results of the 50 simulations in the interval 2.5% - 97.5%. Because the blue line between the distance of 250m-400m are below the gray area, we can infer that the childcare centres in Punggol planning area resemble regular pattern at the distance of 250m-400m."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class_Exercise 1",
    "section": "",
    "text": "Launch the coursework project with RStudio\nCreate a new folder called In-class_Ex.\nCreate a new sub-folder inside the newly created In-class_Ex folder. Name the sub-folder In-class_Ex01.\nCreate a new Quarto document. Save the newly create qmd file in In-class_Ex01 sub-folder. Call the file In-class_Ex01."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "title": "In-class_Exercise 1",
    "section": "Data Processing",
    "text": "Data Processing\nWrite a code chunk to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\n\nCode\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In-class_Exercise 1",
    "section": "Joining popdata2023 and mpsz19_shp",
    "text": "Joining popdata2023 and mpsz19_shp\n\n\nCode\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) \nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\npop2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp, \n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "title": "In-class_Exercise 1",
    "section": "Choropleth Map of Dependency Ratio by Planning Subzone",
    "text": "Choropleth Map of Dependency Ratio by Planning Subzone"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "title": "In-class_Exercise 1",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\n\nThe concept\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n\nStep 1: Data Preparation\nThe code chunk below excludes records with NA by using the code chunk below.\n\n\nCode\nmpsz_pop2023 &lt;- mpsz_pop2023 %&gt;%\n  drop_na()\n\n\n\n\nStep 2: The get function\nThe code chunk below defines a function to get the input data and field to be used for creating the percentile map.\n\n\nCode\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nStep 3: A percentile mapping function\nThe code chunk below creates a function for computing and plotting the percentile map.\n\n\nCode\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(mpsz_pop2023) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nStep 4: Running the functions\n\n\nCode\n#percentmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "title": "In-class_Exercise 1",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\n\nThe Concept\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nStep 1: Creating the boxbreaks function\nThe code chunk on the right is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\nStep 2: Creating the get.var function\nThe code chunk on the right an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nStep 3: Boxmap function\nThe code chunk on the right is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\n\nStep 4: Plotting Box Map\n\n\nCode\n#boxmap(\"DEPENDENCY\", mpsz_pop2023)\n\n\n\n\n\nPlotting Interactive Box Map\n\n\nCode\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nCode\n#boxmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on_Ex03 Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#preparing-the-lixels-objects",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#preparing-the-lixels-objects",
    "title": "Hands-on_Ex03 Network Constrained Spatial Point Patterns Analysis",
    "section": "6.1 Preparing the lixels objects",
    "text": "6.1 Preparing the lixels objects\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode\nlixels &lt;- lixelize_lines(network, \n                         700, \n                         mindist = 375)\n\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#generating-line-centre-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#generating-line-centre-points",
    "title": "Hands-on_Ex03 Network Constrained Spatial Point Patterns Analysis",
    "section": "6.2 Generating line centre points",
    "text": "6.2 Generating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode\nsamples &lt;- lines_center(lixels) \n\n\nThe points are located at center of the line based on the length of the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#performing-nkde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#performing-nkde",
    "title": "Hands-on_Ex03 Network Constrained Spatial Point Patterns Analysis",
    "section": "6.3 Performing NKDE",
    "text": "6.3 Performing NKDE\nWe are ready to computer the NKDE by using the code chunk below.\n\n\nCode\n# densities &lt;- nkde(network, \n#                   events = childcare,\n#                   w = rep(1, nrow(childcare)),\n#                   samples = samples,\n#                   kernel_name = \"quartic\",\n#                   bw = 300, \n#                   div= \"bw\", \n#                   method = \"simple\", \n#                   digits = 1, \n#                   tol = 1,\n#                   grid_shape = c(1,1), \n#                   max_depth = 8,\n#                   agg = 5, \n#                   sparse = TRUE,\n#                  verbose = FALSE)\n\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\n6.3.1 Visualising NKDE\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode\n# samples$density &lt;- densities\n# lixels$density &lt;- densities\n\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n\nCode\n# rescaling to help the mapping\n# samples$density &lt;- samples$density*1000\n# lixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode\n# tmap_mode('view')\n# tm_shape(lixels)+\n#   tm_lines(col=\"density\")+\n# tm_shape(childcare)+\n#   tm_dots()\n# tmap_mode('plot')\n\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html",
    "title": "Hands-on_Ex02A",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#importing-the-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#importing-the-spatial-data",
    "title": "Hands-on_Ex02A",
    "section": "4.1 Importing the spatial data",
    "text": "4.1 Importing the spatial data\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#mapping-the-geospatial-data-sets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#mapping-the-geospatial-data-sets",
    "title": "Hands-on_Ex02A",
    "section": "4.2 Mapping the geospatial data sets",
    "text": "4.2 Mapping the geospatial data sets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\n\nCode\ntmap_mode('view')\n\n\ntmap mode set to interactive viewing\n\n\nCode\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')\n\n\ntmap mode set to plotting\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-on_Ex02A",
    "section": "5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "5.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nCode\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nDIY: Using appropriate function, display the information of these three Spatial* classes as shown below.\n\n\nCode\nchildcare\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\n\nCode\nmpsz\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\n\nCode\nsg\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Hands-on_Ex02A",
    "section": "5.2 Converting the Spatial* class into generic sp format",
    "text": "5.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\n\nCode\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\n\nNext, you should display the sp objects properties as shown below.\n\n\nCode\nchildcare_sp\n\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\nCode\nsg_sp\n\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\nChallenge: Do you know what are the differences between Spatial* classes and generic sp object?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on_Ex02A",
    "section": "5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "5.3 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\n\nWarning in as.ppp.sf(childcare_sf): only first attribute column is used for\nmarks\n\n\nCode\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode\nplot(childcare_ppp)\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#handling-duplicated-points",
    "title": "Hands-on_Ex02A",
    "section": "5.4 Handling duplicated points",
    "text": "5.4 Handling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode\nmultiplicity(childcare_ppp)\n\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\n\nCode\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode\ntmap_mode('view')\n\n\ntmap mode set to interactive viewing\n\n\nCode\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')\n\n\ntmap mode set to plotting\n\n\n\nChallenge: Do you know how to spot the duplicate points from the map shown above?\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nDIY: Using the method you learned in previous section, check if any dusplicated point in this geospatial data.\n\n\nCode\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#creating-owin-object",
    "title": "Hands-on_Ex02A",
    "section": "5.5 Creating owin object",
    "text": "5.5 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe ouput object can be displayed by using plot() function\n\n\nCode\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on_Ex02A",
    "section": "5.6 Combining point events object and owin object",
    "text": "5.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp as shown below."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#kernel-density-estimation",
    "title": "Hands-on_Ex02A",
    "section": "6.1 Kernel Density Estimation",
    "text": "6.1 Kernel Density Estimation\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n6.1.1 Computing kernel density estimation using automatic bandwidth selection method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\n\nCode\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\n\nCode\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n\n   sigma \n298.4095 \n\n\n\n\n6.1.2 Rescalling KDE values\nIn the code chunk below, rescale.ppp() is used to covert the unit of measurement from meter to kilometer.\n\n\nCode\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\n\nCode\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#working-with-different-automatic-badwidth-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#working-with-different-automatic-badwidth-methods",
    "title": "Hands-on_Ex02A",
    "section": "6.2 Working with different automatic badwidth methods",
    "text": "6.2 Working with different automatic badwidth methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n\nCode\n bw.CvL(childcareSG_ppp.km)\n\n\n   sigma \n4.543278 \n\n\n\n\nCode\nbw.scott(childcareSG_ppp.km)\n\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\n\nCode\nbw.ppl(childcareSG_ppp.km)\n\n\n    sigma \n0.3897114 \n\n\n\n\nCode\nbw.diggle(childcareSG_ppp.km)\n\n\n    sigma \n0.2984095 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods.\n\n\nCode\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#working-with-different-kernel-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#working-with-different-kernel-methods",
    "title": "Hands-on_Ex02A",
    "section": "6.3 Working with different kernel methods",
    "text": "6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\n\nCode\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\n\nCode\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#computing-kde-by-using-fixed-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#computing-kde-by-using-fixed-bandwidth",
    "title": "Hands-on_Ex02A",
    "section": "7.1 Computing KDE by using fixed bandwidth",
    "text": "7.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\n\nCode\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#computing-kde-by-using-adaptive-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#computing-kde-by-using-adaptive-bandwidth",
    "title": "Hands-on_Ex02A",
    "section": "7.2 Computing KDE by using adaptive bandwidth",
    "text": "7.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nCode\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nCode\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-kde-output-into-grid-object.",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#converting-kde-output-into-grid-object.",
    "title": "Hands-on_Ex02A",
    "section": "7.3 Converting KDE output into grid object.",
    "text": "7.3 Converting KDE output into grid object.\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\n\nCode\n#gridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\n#spplot(gridded_kde_childcareSG_bw)\n\n\n\n7.3.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\n\nCode\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\n\nCode\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\nNotice that the crs property is NA.\n\n\n7.3.2 Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\n\nCode\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\nNotice that the crs property is completed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#visualising-the-output-in-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#visualising-the-output-in-tmap",
    "title": "Hands-on_Ex02A",
    "section": "7.4 Visualising the output in tmap",
    "text": "7.4 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n\nCode\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-on_Ex02A",
    "section": "7.5 Comparing Spatial Point Patterns using KDE",
    "text": "7.5 Comparing Spatial Point Patterns using KDE\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n7.5.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas\n\n\nCode\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(tm, main = \"Tampines\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(ck, main = \"Choa Chu Kang\")\n\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(jw, main = \"Jurong West\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n7.5.3 Combining childcare points and the study area\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\n\nCode\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 89 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 88 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\n\n\n7.5.4 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\n\n7.5.5 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#testing-spatial-point-patterns-using-clark-and-evans-test",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#testing-spatial-point-patterns-using-clark-and-evans-test",
    "title": "Hands-on_Ex02A",
    "section": "8.1 Testing spatial point patterns using Clark and Evans Test",
    "text": "8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nCode\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nWhat conclusion can you draw from the test result?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "title": "Hands-on_Ex02A",
    "section": "8.2 Clark and Evans Test: Choa Chu Kang planning area",
    "text": "8.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\n\nCode\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.98021, p-value = 0.7674\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#clark-and-evans-test-tampines-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02A.html#clark-and-evans-test-tampines-planning-area",
    "title": "Hands-on_Ex02A",
    "section": "8.3 Clark and Evans Test: Tampines planning area",
    "text": "8.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\n\nCode\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.78952, p-value = 0.0001455\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "",
    "text": "This Hands-on exercise focuses on importing and manipulating geospatial data using the relevant R packages"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-polygon-feature-data-in-shapefile-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-polygon-feature-data-in-shapefile-format",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "3.1 Import polygon feature data in shapefile format",
    "text": "3.1 Import polygon feature data in shapefile format\nThe following code chunk demonstrates how to use the st_read() function from the sf package to import the MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. When working with shapefiles, two arguments are required: dsn to specify the data path and layer to indicate the shapefile name. It’s important to note that file extensions like .shp, .dbf, .prj, and .shx are not needed.\n\n\nCode\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above indicates that the geospatial objects are multipolygon features. There are 323 multipolygon features with 15 associated fields in the MP14_SUBZONE_WEB_PL simple feature data frame. The data is projected using the SVY21 coordinate reference system (CRS). The bounding box details the spatial extent of the data, with the x-axis ranging from 2,667.538 to 56,396.44 and the y-axis ranging from 15,748.72 to 50,256.33."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-polyline-feature-data-in-shapefile-form",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-polyline-feature-data-in-shapefile-form",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "3.2 Import polyline feature data in shapefile form",
    "text": "3.2 Import polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\n\nCode\ncyclingpath = st_read(dsn = \"data/geospatial\", layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 3,138 features and 2 fields in the CyclingPathGazette multiline string feature data frame, and it is also in the SVY21 projected coordinate system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-gis-data-in-kml-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#import-gis-data-in-kml-format",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "3.3 Import GIS data in kml format",
    "text": "3.3 Import GIS data in kml format\nThe PreSchoolsLocation is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\n\nCode\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that PRESCHOOLS_LOCATION is a point feature data frame with a total of 2,290 features and 2 fields. Unlike the previous two simple feature data frames, this one uses the WGS 84 coordinate system. Additionally, the data includes Z-dimension values, with a Z range from 0 to 0."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-st_geometry",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-st_geometry",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "4.1 Working with st_geometry()",
    "text": "4.1 Working with st_geometry()\nThe column in the sf data frame that holds the geometries is a list of class sfc. While you can access the geometry list-column using mpsz$geom or mpsz[[1]], the more general and preferred method is to use the st_geometry() function, as demonstrated in the code chunk below.\n\n\nCode\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNote that the print output only shows basic details of the feature class, including the geometry type, the geographic extent of the features, and the data’s coordinate system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-glimpse",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-glimpse",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "4.2 Working with glimpse()",
    "text": "4.2 Working with glimpse()\nIn addition to the basic feature information, we also want to explore the associated attribute data within the data frame. This is where the glimpse() function from dplyr becomes particularly useful, as demonstrated in the code chunk below.\n\n\nCode\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-head",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#working-with-head",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "4.3 Working with head()",
    "text": "4.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\n\nCode\nhead(mpsz, n=5) \n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "6.1 Assigning EPSG code to a simple feature data frame",
    "text": "6.1 Assigning EPSG code to a simple feature data frame\nA common issue when importing geospatial data into R is that the coordinate system may be either missing (such as when the .proj file for an ESRI shapefile is absent) or incorrectly assigned during the import process.\nThe example below demonstrates how to check the coordinate system of the mpsz simple feature data frame using the st_crs() function from the sf package.\n\n\nCode\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nBased on the results above, although the mpsz data frame is projected in SVY21, the printout indicates that the EPSG code is 9001. This is incorrect because the correct EPSG code for SVY21 should be 3414.\nTo assign the correct EPSG code to the mpsz data frame, you can use the st_set_crs() function from the sf package, as demonstrated in the code chunk below.\n\n\nCode\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nCode\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21.",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21.",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "6.2 Transforming the projection of preschool from wgs84 to svy21.",
    "text": "6.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\n\n\nCode\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#importing-the-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#importing-the-aspatial-data",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "7.1 Importing the aspatial data",
    "text": "7.1 Importing the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\n\nCode\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\n\nRows: 3540 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlist(listings)\n\n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "7.2 Creating a simple feature data frame from an aspatial data frame",
    "text": "7.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\n\nCode\nglimpse(listings_sf)\n\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#buffering",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#buffering",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "8.1 Buffering",
    "text": "8.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\n\nCode\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist=5, nQuadSegs=30)\n\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\n\nCode\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\n\nCode\nsum(buffer_cycling$AREA)\n\n\n2218855 [m^2]\n\n\nMission Completed!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1:Geospatial Data Wrangling with R",
    "section": "8.2 Point-in-polygon count",
    "text": "8.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nCode\nmpsz3414$`PreSch Count` &lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\n\nCode\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nCode\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nThe solution:\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\n\nCode\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nCode\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nprint(mpsz3414)\n\n\nSimple feature collection with 323 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...            0\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...            6\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...            0\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...            5\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...            3\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...           13\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...            5\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...            1\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...           11\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...            1\n              Area    PreSch Density\n1  1630379.3 [m^2]  0.000000 [1/m^2]\n2   559816.2 [m^2] 10.717803 [1/m^2]\n3   160807.5 [m^2]  0.000000 [1/m^2]\n4   595428.9 [m^2]  8.397308 [1/m^2]\n5   387429.4 [m^2]  7.743345 [1/m^2]\n6  1030378.8 [m^2] 12.616719 [1/m^2]\n7   551732.0 [m^2]  9.062370 [1/m^2]\n8   290184.7 [m^2]  3.446082 [1/m^2]\n9  1084792.3 [m^2] 10.140190 [1/m^2]\n10  631644.3 [m^2]  1.583170 [1/m^2]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "",
    "text": "In general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#the-data",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "3.1 The data",
    "text": "3.1 The data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#importing-geospatial-data-into-r",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "3.2 Importing Geospatial Data into R",
    "text": "3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nCode\nmpsz &lt;- st_read(dsn=\"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#importing-attribute-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#importing-attribute-data-into-r",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "3.3 Importing Attribute Data into R",
    "text": "3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nCode\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#data-preparation",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "3.4 Data Preparation",
    "text": "3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`), .groups = 'drop') %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG, \n              values_from = POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[12])) %&gt;%\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15])) %&gt;%\n  mutate(`AGED` = rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL` = rowSums(.[3:21])) %&gt;%  \n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`) / `ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`, \n         `TOTAL`, `DEPENDENCY`)\n\n\n\n\n3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nCode\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nCode\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#plotting-a-choropleth-map-quickly-by-using-qtm",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.1 Plotting a choropleth map quickly by using qtm()",
    "text": "4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#creating-a-choropleth-map-by-using-tmaps-elements",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.2 Creating a choropleth map by using tmap’s elements",
    "text": "4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n4.2.3Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#data-classification-methods-of-tmap",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.3 Data classification methods of tmap",
    "text": "4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\n\n4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nCode\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: Values have found that are higher than the highest break"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#colour-scheme",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.4 Colour Scheme",
    "text": "4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#map-layouts",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.5 Map Layouts",
    "text": "4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\n\nCode\ntmap_style(\"white\")\n\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.6 Drawing Small Multiple Choropleth Maps",
    "text": "4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\n\nCode\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nCode\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#mappping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex01/hands-on_Ex01B.html#mappping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on_Ex01B:Choropleth Mapping with R",
    "section": "4.7 Mappping Spatial Object Meeting a Selection Criterion",
    "text": "4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nCode\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html",
    "title": "Hands-on_Ex02B",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#importing-the-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#importing-the-spatial-data",
    "title": "Hands-on_Ex02B",
    "section": "4.1 Importing the spatial data",
    "text": "4.1 Importing the spatial data\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\n\nCode\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\nReading layer `CostalOutline' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\n\nCode\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#mapping-the-geospatial-data-sets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#mapping-the-geospatial-data-sets",
    "title": "Hands-on_Ex02B",
    "section": "4.2 Mapping the geospatial data sets",
    "text": "4.2 Mapping the geospatial data sets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\n\nCode\ntmap_mode('view')\n\n\ntmap mode set to interactive viewing\n\n\nCode\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')\n\n\ntmap mode set to plotting\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#converting-from-sf-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#converting-from-sf-format-into-spatstats-ppp-format",
    "title": "Hands-on_Ex02B",
    "section": "5.1 Converting from sf format into spatstat’s ppp format",
    "text": "5.1 Converting from sf format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\n\nWarning in as.ppp.sf(childcare_sf): only first attribute column is used for\nmarks\n\n\nCode\nchildcare_ppp\n\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n\nCode\nplot(childcare_ppp)\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\n\nCode\nsummary(childcare_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#handling-duplicated-points",
    "title": "Hands-on_Ex02B",
    "section": "5.2 Handling duplicated points",
    "text": "5.2 Handling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below.\n\n\nCode\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\nCode\nmultiplicity(childcare_ppp)\n\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\n\nCode\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n\nCode\ntmap_mode('view')\n\n\ntmap mode set to interactive viewing\n\n\nCode\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')\n\n\ntmap mode set to plotting\n\n\n\nChallenge: Do you know how to spot the duplicate points from the map shown above?\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\n\nCode\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nDIY: Using the method you learned in previous section, check if any dusplicated point in this geospatial data.\n\n\nCode\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#creating-owin-object",
    "title": "Hands-on_Ex02B",
    "section": "5.3 Creating owin object",
    "text": "5.3 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe ouput object can be displayed by using plot() function\n\n\nCode\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on_Ex02B",
    "section": "5.4 Combining point events object and owin object",
    "text": "5.4 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode\nsummary(childcareSG_ppp)\n\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp as shown below.\n\n5.4.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas\n\n\nCode\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(tm, main = \"Tampines\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(ck, main = \"Choa Chu Kang\")\n\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(jw, main = \"Jurong West\")\n\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Converting sf objects into owin objects\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\n\nCode\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n\n5.4.3 Combining childcare points and the study area\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\n\nCode\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\n\n\nCode\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\n\nCode\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 89 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\n\nCode\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 88 symbols are shown in the symbol map"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area",
    "title": "Hands-on_Ex02B",
    "section": "7.1 Choa Chu Kang planning area",
    "text": "7.1 Choa Chu Kang planning area\n\n7.1.1 Computing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\n\nCode\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n7.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nCode\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(G_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area",
    "title": "Hands-on_Ex02B",
    "section": "7.2 Tampines planning area",
    "text": "7.2 Tampines planning area\n\n7.2.1 Computing G-function estimation\n\n\nCode\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-1",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-1",
    "title": "Hands-on_Ex02B",
    "section": "8.1 Choa Chu Kang planning area",
    "text": "8.1 Choa Chu Kang planning area\n\n8.1.1 Computing F-function estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\n\nCode\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#performing-complete-spatial-randomness-test-2",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#performing-complete-spatial-randomness-test-2",
    "title": "Hands-on_Ex02B",
    "section": "8.2 Performing Complete Spatial Randomness Test",
    "text": "8.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\n\nCode\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(F_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-1",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-1",
    "title": "Hands-on_Ex02B",
    "section": "8.3 Tampines planning area",
    "text": "8.3 Tampines planning area\n\n8.3.1 Computing F-function estimation\nMonte Carlo test with F-fucntion\n\n\nCode\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-2",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-2",
    "title": "Hands-on_Ex02B",
    "section": "9.1 Choa Chu Kang planning area",
    "text": "9.1 Choa Chu Kang planning area\n\n9.1.1 Computing K-fucntion estimate\n\n\nCode\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-2",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-2",
    "title": "Hands-on_Ex02B",
    "section": "9.2 Tampines planning area",
    "text": "9.2 Tampines planning area\n\n9.2.1 Computing K-fucntion estimation\n\n\nCode\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-3",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#choa-chu-kang-planning-area-3",
    "title": "Hands-on_Ex02B",
    "section": "10.1 Choa Chu Kang planning area",
    "text": "10.1 Choa Chu Kang planning area\n\n10.1.1 Computing L Fucntion estimation\n\n\nCode\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n\n10.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-3",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02B.html#tampines-planning-area-3",
    "title": "Hands-on_Ex02B",
    "section": "10.2 Tampines planning area",
    "text": "10.2 Tampines planning area\n\n10.2.1 Computing L-fucntion estimate\n\n\nCode\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n\n10.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\n\nCode\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nThen, plot the model output by using the code chun below.\n\n\nCode\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "ISSS626-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class_Ex02",
    "section": "",
    "text": "Code\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#introducing-spatstat-package",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#introducing-spatstat-package",
    "title": "In-class_Ex02",
    "section": "Introducing spatstat package",
    "text": "Introducing spatstat package\nspatstat R package is a comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype or marked points, in any spatial region."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#spatstat",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#spatstat",
    "title": "In-class_Ex02",
    "section": "spatstat",
    "text": "spatstat\n\nspatstat sub-packages\n\nThe spatstat package now contains only documentation and introductory material. It provides beginner’s introductions, vignettes, interactive demonstration scripts, and a few help files summarising the package.\nThe spatstat.data package now contains all the datasets for spatstat.\nThe spatstat.utils package contains basic utility functions for spatstat.\nThe spatstat.univar package contains functions for estimating and manipulating probability distributions of one-dimensional random variables.\nThe spatstat.sparse package contains functions for manipulating sparse arrays and performing linear algebra.\nThe spatstat.geom package contains definitions of spatial objects (such as point patterns, windows and pixel images) and code which performs geometrical operations.\nThe spatstat.random package contains functions for random generation of spatial patterns and random simulation of models.\nThe spatstat.explore package contains the code for exploratory data analysis and nonparametric analysis of spatial data.\nThe spatstat.model package contains the code for model-fitting, model diagnostics, and formal inference.\nThe spatstat.linnet package defines spatial data on a linear network, and performs geometrical operations and statistical analysis on such data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-ppp-objects-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-ppp-objects-from-sf-data.frame",
    "title": "In-class_Ex02",
    "section": "Creating ppp objects from sf data.frame",
    "text": "Creating ppp objects from sf data.frame\nInstead of using the two steps approaches discussed in Hands-on Exercise 3 to create the ppp objects, in this section you will learn how to work with sf data.frame.\nIn the code chunk below, as.ppp() of spatstat.geom package is used to derive an ppp object layer directly from a sf tibble data.frame.\n\n\nCode\n#childcare_ppp &lt;- as.ppp(childcare_sf)\n#plot(childcare_ppp)\n\n\nNext, summary() can be used to reveal the properties of the newly created ppp objects.\n\n\nCode\n#summary(childcare_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-owin-object-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#creating-owin-object-from-sf-data.frame",
    "title": "In-class_Ex02",
    "section": "Creating owin object from sf data.frame",
    "text": "Creating owin object from sf data.frame\nIn the code chunk as.owin() of spatstat.geom is used to create an owin object class from polygon sf tibble data.frame.\n\n\nCode\n#sg_owin &lt;- as.owin(sg_sf)\n#plot(sg_owin)\n\n\nNext, summary() function is used to display the summary information of the owin object class.\n\n\nCode\n#summary(sg_owin)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#combining-point-events-object-and-owin-object",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#combining-point-events-object-and-owin-object",
    "title": "In-class_Ex02",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\nUsing the step you learned from Hands-on Exercise 3, create an ppp object by combining childcare_ppp and sg_owin.\n\n\nCode\n#childcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode\n#plot(childcareSG_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-of-spatial-point-event",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation-of-spatial-point-event",
    "title": "In-class_Ex02",
    "section": "Kernel Density Estimation of Spatial Point Event",
    "text": "Kernel Density Estimation of Spatial Point Event\nThe code chunk below re-scale the unit of measurement from metre to kilometre before performing KDE.\n\n\nCode\n# #childcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, \n#                                   1000, \n#                                   \"km\")\n# \n# #kde_childcareSG_adaptive &lt;- adaptive.density(\n#   childcareSG_ppp.km, \n#   method=\"kernel\")\n# plot(kde_childcareSG_adaptive)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#kernel-density-estimation",
    "title": "In-class_Ex02",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nCode chunk shown two different ways to convert KDE output into grid object\n\n\nCode\n# par(bg = '#E4D5C9')\n# \n# gridded_kde_childcareSG_ad &lt;- maptools::as.SpatialGridDataFrame.im(\n#   kde_childcareSG_adaptive)\n# spplot(gridded_kde_childcareSG_ad)\n\n\n\n\nCode\n# gridded_kde_childcareSG_ad &lt;- as(\n#   kde_childcareSG_adaptive,\n#   \"SpatialGridDataFrame\")\n# spplot(gridded_kde_childcareSG_ad)\n\n\n\n\nVisualising KDE using tmap\nThe code chunk below is used to plot the output raster by using tmap functions.\n\n\nCode\n# tm_shape(kde_childcareSG_ad_raster) + \n#   tm_raster(palette = \"viridis\") +\n#   tm_layout(legend.position = c(\"right\", \"bottom\"), \n#             frame = FALSE,\n#             bg.color = \"#E4D5C9\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-study-area-using-sf-objects",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-study-area-using-sf-objects",
    "title": "In-class_Ex02",
    "section": "Extracting study area using sf objects",
    "text": "Extracting study area using sf objects\nExtract and create an ppp object showing child care services and within Punggol Planning Area\nOn the other hand, filter() of dplyr package should be used to extract the target planning areas as shown in the code chunk below.\n\n\nCode\n# pg_owin &lt;- mpsz_sf %&gt;%\n#   filter(PLN_AREA_N == \"PUNGGOL\") %&gt;%\n#   as.owin()\n# \n# childcare_pg = childcare_ppp[pg_owin]\n# \n# plot(childcare_pg)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#monte-carlo-simulation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#monte-carlo-simulation",
    "title": "In-class_Ex02",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nIn order to ensure reproducibility, it is important to include the code chunk below before using spatstat functions involve Monte Carlo simulation\n\n\nCode\n# set.seed(1234)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#edge-correction-methods-of-spatstat",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#edge-correction-methods-of-spatstat",
    "title": "In-class_Ex02",
    "section": "Edge correction methods of spatstat",
    "text": "Edge correction methods of spatstat\nIn spatstat, edge correction methods are used to handle biases that arise when estimating spatial statistics near the boundaries of a study region. These corrections are essential for ensuring accurate estimates in spatial point pattern analysis, especially for summary statistics like the K-function, L-function, pair correlation function, etc.\nCommon Edge Correction Methods in spatstat\n\n“none”: No edge correction is applied. This method assumes that there is no bias at the edges, which may lead to underestimation of statistics near the boundaries.\n“isotropic”: This method corrects for edge effects by assuming that the point pattern is isotropic (uniform in all directions). It compensates for missing neighbors outside the boundary by adjusting the distances accordingly.\n“translate” (Translation Correction): This method uses a translation correction, which involves translating the observation window so that every point lies entirely within it. The statistic is then averaged over all possible translations.\n“Ripley” (Ripley’s Correction): Similar to the isotropic correction but specifically tailored for Ripley’s K-function and related functions. It adjusts the expected number of neighbors for points near the edges based on the shape and size of the observation window.\n“border”: Border correction reduces bias by only considering points far enough from the boundary so that their neighborhood is fully contained within the window. This can be quite conservative but reduces the influence of edge effects."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "title": "In-class_Ex02",
    "section": "Geospatial Analytics for Social Good: Thailand Road Accident Case Study",
    "text": "Geospatial Analytics for Social Good: Thailand Road Accident Case Study\n\nBackground\n\nRoad traffic injuries, WHO.\nRoad traffic deaths and injuries in Thailand"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#the-study-area",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#the-study-area",
    "title": "In-class_Ex02",
    "section": "The Study Area",
    "text": "The Study Area\nThe study area is Bangkok Metropolitan Region.\nThe projected coordinate system of Thailand is WGS 84 / UTM zone 47N and the EPSG code is 32647."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#the-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#the-data",
    "title": "In-class_Ex02",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this exercise, three basic data sets are needed, they are:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-traffic-accident-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-traffic-accident-data",
    "title": "In-class_Ex02",
    "section": "Importing Traffic Accident Data",
    "text": "Importing Traffic Accident Data\nUsing the steps you learned in previous lessons, import the downloaded accident data into R environment and save the output as an sf tibble data.frame.\n\n\nCode\n# rdacc_sf &lt;- read_csv(\"data/thai_road_accident_2019_2022.csv\") %&gt;%\n#   filter(!is.na(longitude) & longitude != \"\", \n#          !is.na(latitude) & latitude != \"\") %&gt;%\n#   st_as_sf(coords = c(\n#     \"longitude\", \"latitude\"),\n#     crs=4326) %&gt;%\n#   st_transform(crs = 32647)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#visualising-the-accident-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#visualising-the-accident-data",
    "title": "In-class_Ex02",
    "section": "Visualising The Accident Data",
    "text": "Visualising The Accident Data\nUsing the steps you learned in previous lessons, import the ACLED data into R environment as an sf tibble data.frame.\n\n\nCode\n# tmap_mode(\"plot\")\n# acled_sf %&gt;%\n#   filter(year == 2023 | \n#            event_type == \"Political violence\") %&gt;%\n#   tm_shape()+\n#   tm_dots()\n# tmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "In this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\nUsing the step you leanred from previous hands-in, install and load the necessary R packages in R environment.\n\n\nCode\npacman::p_load(sf,ggstatsplot, tmap, tidyverse, knitr, GWmodel)\n\n\nInstalling package into 'C:/Users/cttdn/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'glue', 'rlang'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'glue' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'glue'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\glue\\libs\\x64\\glue.dll to\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\glue\\libs\\x64\\glue.dll:\nPermission denied\n\n\nWarning: restored 'glue'\n\n\npackage 'rlang' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'rlang'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\rlang\\libs\\x64\\rlang.dll\nto C:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\rlang\\libs\\x64\\rlang.dll:\nPermission denied\n\n\nWarning: restored 'rlang'\n\n\npackage 'ggstatsplot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\cttdn\\AppData\\Local\\Temp\\Rtmpq2jcih\\downloaded_packages\n\n\n\nggstatsplot installed\n\n\nWarning: package 'ggstatsplot' was built under R version 4.3.3\n\n\nWarning in pacman::p_load(sf, ggstatsplot, tmap, tidyverse, knitr, GWmodel): Failed to install/load:\nggstatsplot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#loading-the-package",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#loading-the-package",
    "title": "In-class_Ex04",
    "section": "",
    "text": "In this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\nUsing the step you leanred from previous hands-in, install and load the necessary R packages in R environment.\n\n\nCode\npacman::p_load(sf,ggstatsplot, tmap, tidyverse, knitr, GWmodel)\n\n\nInstalling package into 'C:/Users/cttdn/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'glue', 'rlang'\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'glue' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'glue'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\glue\\libs\\x64\\glue.dll to\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\glue\\libs\\x64\\glue.dll:\nPermission denied\n\n\nWarning: restored 'glue'\n\n\npackage 'rlang' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'rlang'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\00LOCK\\rlang\\libs\\x64\\rlang.dll\nto C:\\Users\\cttdn\\AppData\\Local\\R\\win-library\\4.3\\rlang\\libs\\x64\\rlang.dll:\nPermission denied\n\n\nWarning: restored 'rlang'\n\n\npackage 'ggstatsplot' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\cttdn\\AppData\\Local\\Temp\\Rtmpq2jcih\\downloaded_packages\n\n\n\nggstatsplot installed\n\n\nWarning: package 'ggstatsplot' was built under R version 4.3.3\n\n\nWarning in pacman::p_load(sf, ggstatsplot, tmap, tidyverse, knitr, GWmodel): Failed to install/load:\nggstatsplot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#preparing-the-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#preparing-the-data",
    "title": "In-class_Ex04",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nFor this in-class exercise, Hunan shapefile and Hunan_2012 data file will be used.Using the steps you learned from previous hands-on, complete the following tasks:\n\nimport Hunan shapefile and parse it into a sf polygon feature object.\nimport Hunan_2012.csv file parse it into a tibble data.frame.\njoin Hunan and Hunan_2012 data.frames.\n\n\n\nCode\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\In-class_Ex\\In-class_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# hunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n#   select(1:3, 7,15, 16, 31,32)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "title": "In-class_Ex04",
    "section": "Mapping GDPPC",
    "text": "Mapping GDPPC\nUsing the steps you learned from Hands-on Exercise 5, prepare a choropleth map showing the geographic distribution of GDPPC of Hunan Province.\n\n\nCode\n# basemap &lt;- tm_shape(hunan_sf) +\n#   tm_polygons() +\n#   tm_text(\"NAME_3\", size=0.5)\n# \n# gdppc &lt;- qtm(hunan_sf, \"GDPPC\")\n# tmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "title": "In-class_Ex04",
    "section": "Converting to SpatialPolygonDataFrame",
    "text": "Converting to SpatialPolygonDataFrame\n\n\nCode\n# hunan_sp &lt;- hunan_sf %&gt;%\n#   as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "title": "In-class_Ex04",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\n\nDetermine adaptive bandwidth\n\n\nCode\n# bw_CV &lt;- bw.gwr(GDPPC ~ 1,\n#                 data = hunan_sp,\n#                 approach = \"CV\",\n#                 adaptive = TRUE,\n#                 kernel = \"bisquare\",\n#                 longlat = T)\n\n\n\n\nCode\n# bw_CV\n\n\n\n\nCode\n# bw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n#                 data = hunan_sp,\n#                 approach = \"AIC\",\n#                 adaptive = TRUE,\n#                 kernel = \"bisquare\",\n#                 longlat = T)\n\n\n\n\nCode\n# bw_AIC"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "title": "In-class_Ex04",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\n\nComputing geographically wieghted summary statistics\n# gwstat &lt;- gwss(data = hunan_sp,\n#                vars = \"GDPPC\",\n#                bw = bw_AIC,\n#                kernel = \"bisquare\",\n#                adaptive = TRUE,\n#                longlat = T)\n\n\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\nCode\n# gwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nCode\n# hunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-geographically-weighted-summary-statistics",
    "title": "In-class_Ex04",
    "section": "Visualising geographically weighted summary statistics",
    "text": "Visualising geographically weighted summary statistics\n\n\nCode\n# tm_shape(hunan_gstat) +\n#   tm_fill(\"GDPPC_LM\",\n#           n = 5,\n#           style = \"quantile\") +\n#   tm_borders(alpha = 0.5) +\n#   tm_layout(main.title = \"Distribution of Geographically weighted mean\",\n#             main.title.position = \"center\",\n#             main.title.size = 1.0,\n#             legend.text.size = 0.5,\n#             legend.height = 0.5,\n#             legend.width = 0.5,\n#             frame = TRUE)\n\n\n\n\nDetermine fixed bandwidth\n\n\nCode\n# bw_CV &lt;- bw.gwr(GDPPC ~ 1,\n#                 data = hunan_sp,\n#                 approach = \"CV\",\n#                 adaptive = FALSE,\n#                 kernel = \"bisquare\",\n#                 longlat = T)\n\n\n\n\nCode\n# bw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n#                 data = hunan_sp,\n#                 approach = \"AIC\",\n#                 adaptive = FALSE,\n#                 kernel = \"bisquare\",\n#                 longlat = T)\n\n\n\n\nComputing adaptive bandwidth\n\n\nCode\n# gwstat &lt;- gwss(data = hunan_sp,\n#                vars = \"GDPPC\",\n#                bw = bw_AIC,\n#                kernel = \"bisquare\",\n#                adaptive = FALSE,\n#                longlat = T)\n\n\n\n\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\nCode\n# gwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nCode\n# hunan_gstat &lt;- cbind(hunan_sf, gwstat_df)\n\n\n\n\nVisualising geographically weighted summary statistics\n\n\nCode\n# tm_shape(hunan_gstat) +\n#   tm_fill(\"GDPPC_LM\",\n#           n = 5,\n#           style = \"quantile\") +\n#   tm_borders(alpha = 0.5) +\n#   tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n#             main.title.position = \"center\",\n#             main.title.size = 2.0,\n#             legend.text.size = 1.2,\n#             legend.height = 1.50, \n#             legend.width = 1.50,\n#             frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "title": "In-class_Ex04",
    "section": "Geographically Weighted Correlation with Adaptive Bandwidth",
    "text": "Geographically Weighted Correlation with Adaptive Bandwidth\nBusiness question: Is there any relationship between GDP per capita and Gross Industry Output?\n\n\nCode\n# ggscatterstats(\n#   data = hunan2012, \n#   x = Agri, \n#   y = GDPPC,\n#   xlab = \"Gross Agriculture Output\", ## label for the x-axis\n#   ylab = \"GDP per capita\", \n#   label.var = County, \n#   label.expression = Agri &gt; 10000 & GDPPC &gt; 50000, \n#   point.label.args = list(alpha = 0.7, size = 4, color = \"grey50\"),\n#   xfill = \"#CC79A7\", \n#   yfill = \"#009E73\", \n#   title = \"Relationship between GDP PC and Gross Agriculture Output\")\n\n\n\nGeospatial analytics solution\n\n\nCode\n# bw &lt;- bw.gwr(GDPPC ~ GIO, \n#              data = hunan_sp, \n#              approach = \"AICc\", \n#              adaptive = TRUE)\n\n\n\n\nCode\n# gwstats &lt;- gwss(hunan_sp, \n#                 vars = c(\"GDPPC\", \"GIO\"), \n#                 bw = bw,\n#                 kernel = \"bisquare\",\n#                 adaptive = TRUE, \n#                 longlat = T)\n\n\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\nCode\n# gwstat_df &lt;- as.data.frame(gwstats$SDF) %&gt;%\n#   select(c(12,13)) %&gt;%\n#   rename(gwCorr = Corr_GDPPC.GIO,\n#          gwSpearman = Spearman_rho_GDPPC.GIO)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nCode\n# hunan_Corr &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-local-correlation",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-local-correlation",
    "title": "In-class_Ex04",
    "section": "Visualising Local Correlation",
    "text": "Visualising Local Correlation\n\n\nCode\n# tm_shape(hunan_Corr) +\n#   tm_fill(\"gwSpearman\",\n#           n = 5,\n#           style = \"quantile\") +\n#   tm_borders(alpha = 0.5) +\n#   tm_layout(main.title = \"Local Spearman Rho\",\n#             main.title.position = \"center\",\n#             main.title.size = 2.0,\n#             legend.text.size = 1.2,\n#             legend.height = 1.50, \n#             legend.width = 1.50,\n#             frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "In-class_Ex06",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\nIt is highly recommended to read Emerging Hot Spot Analysis before you continue the exercise."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#overview",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#overview",
    "title": "In-class_Ex06",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\nIt is highly recommended to read Emerging Hot Spot Analysis before you continue the exercise."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#getting-started",
    "title": "In-class_Ex06",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling and Loading the R Packages\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, plotly, and tidyverse.\n\n\nCode\npacman::p_load(sf, sfdep, tmap, \n               plotly, tidyverse, \n               Kendall)\n\n\nInstalling package into 'C:/Users/cttdn/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\npackage 'Kendall' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\cttdn\\AppData\\Local\\Temp\\RtmpghW2Og\\downloaded_packages\n\n\n\nKendall installed\n\n\nWarning: package 'Kendall' was built under R version 4.3.3"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#the-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#the-data",
    "title": "In-class_Ex06",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_GDPPC, an attribute data set in csv format.\n\nBefore getting started, reveal the content of Hunan_GDPPC.csv by using Notepad and MS Excel.\n\nImporting geospatial data\nIn the code chunk below, st_read() of sf package is used to import Hunan shapefile into R.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\In-class_Ex\\In-class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting attribute table\nIn the code chunk below, read_csv() of readr is used to import Hunan_GDPPC.csv into R.\n\n\nCode\nGDPPC &lt;- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\nRows: 1496 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\ndbl (2): Year, GDPPC\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#creating-a-time-series-cube",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#creating-a-time-series-cube",
    "title": "In-class_Ex06",
    "section": "Creating a Time Series Cube",
    "text": "Creating a Time Series Cube\nBefore getting started, students must read this article to learn the basic concept of spatio-temporal cube and its implementation in sfdep package.\nIn the code chunk below, spacetime() of sfdep ised used to create an spatio-temporal cube.\n\n\nCode\nGDPPC_st &lt;- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n\nNext, is_spacetime_cube() of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object.\nThe TRUE return confirms that GDPPC_st object is indeed an time-space cube."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-gi",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-gi",
    "title": "In-class_Ex06",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nNext, we will compute the local Gi* statistics.\n\nDeriving the spatial weights\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\n\nCode\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\nNote that this dataset now has neighbors and weights for each time-slice."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-gi-1",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#computing-gi-1",
    "title": "In-class_Ex06",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n\nCode\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#mann-kendall-test",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#mann-kendall-test",
    "title": "In-class_Ex06",
    "section": "Mann-Kendall Test",
    "text": "Mann-Kendall Test\nA monotonic series or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.\nH0: No monotonic trend\nH1: Monotonic trend is present\nInterpretation\n\nReject the null-hypothesis null if the p-value is smaller than the alpha value (i.e. 1-confident level)\nTau ranges between -1 and 1 where:\n\n-1 is a perfectly decreasing series, and\n1 is a perfectly increasing series.\n\n\nYou are encouraged to read Mann-Kendall Test For Monotonic Trend to learn more about the concepts and method of Mann-Kendall test..\n\nMann-Kendall Test on Gi\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.\n\n\nCode\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(County == \"Changsha\") |&gt; \n  select(County, Year, gi_star)\n\n\nNext, we plot the result by using ggplot2 functions.\n\n\nCode\nggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Mann-Kendall Plot\nWe can also create an interactive plot by using ggplotly() of plotly package.\n\n\nCode\np &lt;- ggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nPrinting Mann-Kendall test report\n\n\nCode\ncbg %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.485 0.00742    66  136.  589.\n\n\nIn the above result, sl is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.\n\n\nMann-Kendall test data.frame\nWe can replicate this for each location by using group_by() of dplyr package.\n\n\nCode\nehsa &lt;- gi_stars %&gt;%\n  group_by(County) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n\n\n# A tibble: 6 × 6\n  County        tau        sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Anhua      0.191  0.303        26  136.  589.\n2 Anren     -0.294  0.108       -40  136.  589.\n3 Anxiang    0      1             0  136.  589.\n4 Baojing   -0.691  0.000128    -94  136.  589.\n5 Chaling   -0.0882 0.650       -12  136.  589.\n6 Changning -0.750  0.0000318  -102  136.  589.\n\n\n\n\nMann-Kendall test data.frame\nWe can also sort to show significant emerging hot/cold spots\n\n\nCode\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:10)\nhead(emerging)\n\n\n# A tibble: 6 × 6\n  County        tau         sl     S     D  varS\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Shuangfeng  0.868 0.00000143   118  136.  589.\n2 Xiangtan    0.868 0.00000143   118  136.  589.\n3 Xiangxiang  0.868 0.00000143   118  136.  589.\n4 Chengbu    -0.824 0.00000482  -112  136.  589.\n5 Dongan     -0.824 0.00000482  -112  136.  589.\n6 Wugang     -0.809 0.00000712  -110  136.  589."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html#performing-emerging-hotspot-analysis",
    "title": "In-class_Ex06",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\n\nCode\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n\n\n\nVisualising the distribution of EHSA classes\nIn the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.\n\n\nCode\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nFigure above shows that sporadic cold spots class has the high numbers of county.\n\n\nVisualising EHSA\nIn this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both hunan and ehsa together by using the code chunk below.\n\n\nCode\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\n\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n\n\nCode\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of EHSA classes\n\n\n\nInterpretation of EHSA classes\n\n\n\nInterpretation of EHSA classes\n\n\nInterpretation of EHSA classes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS626-Geospatial Analytics and Applications",
    "section": "",
    "text": "Welcome to ISSS626 Geospatial Analytics and Applications. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "title": "Take-Home_Exercise 1",
    "section": "",
    "text": "Road traffic accidents are a global health concern, responsible for approximately 1.19 million deaths annually and leaving 20–50 million people injured. Vulnerable road users—pedestrians, cyclists, and motorcyclists—are disproportionately affected. In Thailand, recognized as having the deadliest roads in Southeast Asia, about 20,000 people die annually in traffic accidents, with accidents peaking on national highways and accident-prone zones termed “black spots.”"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#the-study-area",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#the-study-area",
    "title": "Take-Home_Exercise 1",
    "section": "2.1 The study area",
    "text": "2.1 The study area\nThe focus of this study would be in Bangkok Metropolitan Region, Thailand."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#install-and-launching-r-packages.",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#install-and-launching-r-packages.",
    "title": "Take-Home_Exercise 1",
    "section": "2.2 Install and Launching R packages.",
    "text": "2.2 Install and Launching R packages.\nIn this take Home exercise 1, multiple R packages will be used to perform various geospatial and statistical analyses, including spatial point pattern analysis, data manipulation, and visualization. The required R packages are as follows:\n\nspNetwork: This package provides functions for conducting Spatial Point Patterns Analysis, such as kernel density estimation (KDE) and K-function on networks. It is also useful for building spatial matrices and traditional spatial analysis methods based on reticular distances.\nsf: Provides functions to manage, process, and manipulate Simple Features, a formal geospatial data standard that specifies the storage and access model of spatial geometries, including points, lines, and polygons.\ntmap: Offers functions for plotting cartographic-quality maps and interactive maps, leveraging the Leaflet API for interactive visualizations.\ntidyverse: A collection of packages designed for data science, including tools for data manipulation (dplyr), tidying (tidyr), and visualization (ggplot2).\nspatstat: A package for analyzing spatial point patterns, offering tools for modeling spatial data.\nggplot2: A system for declaratively creating graphics, based on the grammar of graphics.\nleaflet: Facilitates the creation of interactive maps directly in R, supporting a variety of map styles and data layers.\ndplyr: Part of the tidyverse, providing powerful data manipulation functions.\nlubridate: Aids in dealing with date-time data, making it easier to parse and manipulate temporal data.\ndbscan: Used for density-based clustering of spatial data, often applied in identifying clusters of points, such as accident-prone areas.\nigraph: A package for network analysis, offering tools to visualize and analyze the properties of spatial networks.\n\nUse the following code to install and launch all necessary R packages:\n\npacman::p_load(sf, tidyverse, tmap, spNetwork, spatstat,ggplot2, leaflet, dplyr, lubridate, dbscan, igraph, dodgr,future)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#filtering-data-for-bangkok-metropolitan-region-bmr",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#filtering-data-for-bangkok-metropolitan-region-bmr",
    "title": "Take-Home_Exercise 1",
    "section": "3.1 Filtering Data for Bangkok Metropolitan Region (BMR)",
    "text": "3.1 Filtering Data for Bangkok Metropolitan Region (BMR)\nThis code filters the road accident dataset (acc) to include only accidents that occurred within the Bangkok Metropolitan Region (BMR). The filtering is done by selecting specific provinces from the province_en column that correspond to the BMR area:\n\nBangkok\nNonthaburi\nNakhon Pathom\nPathum Thani\nSamut Prakan\nSamut Sakhon\n\nThe result is stored in a new dataset called BMR_acc, which contains only the accident records from these provinces. This step ensures that subsequent analysis focuses exclusively on the BMR region, aligning with the objectives of the geospatial analysis task.\n\nBMR_acc &lt;- acc %&gt;%\n  filter(province_en %in% c(\"Bangkok\", \"Nonthaburi\", \"Nakhon Pathom\", \n                            \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#saving-the-filtered-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#saving-the-filtered-data",
    "title": "Take-Home_Exercise 1",
    "section": "3.2 Saving the Filtered Data",
    "text": "3.2 Saving the Filtered Data\nIn this step, the filtered dataset BMR_acc is saved to an RDS file using the write_rds() function. The file is saved at the specified path: \"data/rds/BMR_acc_data.rds\".\nThis allows the user to store the dataset and load it later for further analysis without needing to filter the data again. The RDS format is efficient for saving R objects, preserving both the structure and data of the BMR_acc dataset.\n\nwrite_rds(BMR_acc, \"data/rds/BMR_acc_data.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-the-filtered-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-the-filtered-data",
    "title": "Take-Home_Exercise 1",
    "section": "3.3 Loading the Filtered Data",
    "text": "3.3 Loading the Filtered Data\nIn this step, the saved RDS file containing the filtered Bangkok Metropolitan Region accident data (BMR_acc_data.rds) is loaded back into R using the read_rds() function. The data is assigned to the object BMR_acc_data.\nThis allows for easy access to the previously saved dataset without the need to repeat the filtering and transformation steps, making the workflow more efficient. The file path \"data/rds/BMR_acc_data.rds\" points to the location where the data was saved in the earlier step.\n\nBMR_acc_data &lt;- read_rds(\"data/rds/BMR_acc_data.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-road-network-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-road-network-data",
    "title": "Take-Home_Exercise 1",
    "section": "3.4 Loading Road Network Data",
    "text": "3.4 Loading Road Network Data\nIn this step, the road network data is being read into R using the st_read() function from the sf package. This function reads spatial data from a shapefile.\n\nThe dsn parameter specifies the directory where the raw data is located: \"data/rawdata\".\nThe layer parameter identifies the specific shapefile layer being loaded: \"hotosm_tha_roads_lines_shp\".\n\nThe result is stored in the object thai_one_map, which will contain the road network data in a spatial format, ready for spatial analysis or mapping tasks.\nThis step ensures that the road network can be used alongside the accident data for conducting network-based spatial point pattern analysis.\n\nthai_one_map &lt;- st_read(dsn = \"data/rawdata\",\n                        layer = \"hotosm_tha_roads_lines_shp\")\n\nReading layer `hotosm_tha_roads_lines_shp' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2792590 features and 14 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 97.34457 ymin: 5.643645 xmax: 105.6528 ymax: 20.47168\nCRS:           NA"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-administrative-boundaries-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#loading-administrative-boundaries-data",
    "title": "Take-Home_Exercise 1",
    "section": "3.5 Loading Administrative Boundaries Data",
    "text": "3.5 Loading Administrative Boundaries Data\nIn this step, administrative boundary data is loaded into R using the st_read() function from the sf package. This function reads spatial data from a shapefile, and the data is stored in the admin_boundaries object.\n\nThe dsn parameter points to the directory containing the shapefile: \"data/rawdata\".\nThe layer parameter specifies the specific shapefile layer to be loaded: \"tha_admbnda_adm1_rtsd_20220121\".\n\nThis dataset likely contains the administrative boundaries of Thailand at the first administrative level, which could be provinces or regions. These boundaries can be used for geospatial analysis or visualization alongside the road network and accident data, providing a spatial context for the analysis.\n\nadmin_boundaries &lt;- st_read(dsn = \"data/rawdata\",\n                        layer = \"tha_admbnda_adm1_rtsd_20220121\")\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `D:\\SMUJunJie\\ISSS626-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#filtering-administrative-boundaries-for-bmr-provinces",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#filtering-administrative-boundaries-for-bmr-provinces",
    "title": "Take-Home_Exercise 1",
    "section": "3.6 Filtering Administrative Boundaries for BMR Provinces",
    "text": "3.6 Filtering Administrative Boundaries for BMR Provinces\nIn this step, the administrative boundaries for each province within the Bangkok Metropolitan Region (BMR) are filtered from the admin_boundaries dataset. The filter() function is used to select boundaries for specific provinces by matching the ADM1_EN column (which contains the English names of provinces) with the relevant province names. The filtered boundaries are stored in separate objects for each province:\n\nBangkok: Stored in bangkok_boundary.\nNonthaburi: Stored in nonthaburi_boundary.\nNakhon Pathom: Stored in nakhon_pathom_boundary.\nPathum Thani: Stored in pathum_thani_boundary.\nSamut Prakan: Stored in samut_prakan_boundary.\nSamut Sakhon: Stored in samut_sakhon_boundary.\n\nThese boundary objects can be used for geospatial visualization or analysis, allowing for province-specific analysis or mapping of road accidents within each of these regions.\n\nbangkok_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Bangkok\")\n\nNonthaburi_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Nonthaburi\")\n\nNakhon_Pathom_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Nakhon Pathom\")\n\nPathum_Thani_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Pathum Thani\")\n\nSamut_Prakan_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Samut Prakan\")\n\nSamut_Sakhon_boundary &lt;- admin_boundaries %&gt;%\n  filter(ADM1_EN == \"Samut Sakhon\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#setting-coordinate-reference-system-crs",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#setting-coordinate-reference-system-crs",
    "title": "Take-Home_Exercise 1",
    "section": "3.7Setting Coordinate Reference System (CRS)",
    "text": "3.7Setting Coordinate Reference System (CRS)\nIn this step, the Coordinate Reference System (CRS) of the thai_one_map dataset is set to EPSG: 4326 using the st_crs() function from the sf package.\n\nEPSG: 4326 is the geographic coordinate system based on the World Geodetic System 1984 (WGS 84), which represents data using latitude and longitude.\n\nThis step ensures that the spatial data in thai_one_map is correctly projected and can be aligned with other datasets or maps using the same CRS, such as accident data or administrative boundaries.\n\nst_crs(thai_one_map) &lt;- 4326"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#intersecting-road-network-with-administrative-boundaries",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#intersecting-road-network-with-administrative-boundaries",
    "title": "Take-Home_Exercise 1",
    "section": "3.8 Intersecting Road Network with Administrative Boundaries",
    "text": "3.8 Intersecting Road Network with Administrative Boundaries\nIn this step, the road network data (thai_one_map) is intersected with the administrative boundaries of each province in the Bangkok Metropolitan Region (BMR) to isolate the road segments within each province. The st_intersection() function is used to perform these spatial intersections.\n\nBangkok roads: Intersection between thai_one_map and bangkok_boundary, resulting in road segments within Bangkok.\nNonthaburi roads: Intersection between thai_one_map and nonthaburi_boundary.\nNakhon Pathom roads: Intersection between thai_one_map and nakhon_pathom_boundary.\nPathum Thani roads: Intersection between thai_one_map and pathum_thani_boundary.\nSamut Prakan roads: Intersection between thai_one_map and samut_prakan_boundary.\nSamut Sakhon roads: Intersection between thai_one_map and samut_sakhon_boundary.\n\nThis step allows for focusing the road network analysis on each specific province within BMR. It prepares the data for further spatial analysis or visualization of road accidents within each region.\n\nBMR_acc &lt;- acc %&gt;%\n  filter(province_en %in% c(\"Bangkok\", \"Nonthaburi\", \"Nakhon Pathom\", \n                            \"Pathum Thani\", \"Samut Prakan\", \"Samut Sakhon\"))\n\nBangkok_roads &lt;- st_intersection(thai_one_map, bangkok_boundary)\n\nNonthaburi_roads &lt;- st_intersection(thai_one_map, Nonthaburi_boundary)\n\nNakhon_Pathom_roads &lt;- st_intersection(thai_one_map, Nakhon_Pathom_boundary)\n\nPathum_Thani_roads &lt;- st_intersection(thai_one_map, Pathum_Thani_boundary)\n\nSamut_Prakan_roads &lt;- st_intersection(thai_one_map, Samut_Prakan_boundary)\n\nSamut_Sakhon_roads &lt;- st_intersection(thai_one_map, Samut_Sakhon_boundary)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#enhancing-accident-data-with-date-components",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#enhancing-accident-data-with-date-components",
    "title": "Take-Home_Exercise 1",
    "section": "4.1 Enhancing Accident Data with Date Components",
    "text": "4.1 Enhancing Accident Data with Date Components\nThis R code snippet enhances the accident data (BMR_acc_data) by extracting additional date components from the incident_datetime column. The mutate() function is used to create new columns:\n\nincident_date: Converts incident_datetime to a date format using as.Date().\nyear: Extracts the year from incident_datetime.\nmonth: Extracts the month from incident_datetime and labels it with the month name using month(..., label = TRUE).\nday_of_week: Extracts the day of the week from incident_datetime and labels it with the weekday name using wday(..., label = TRUE).\n\nThese transformations provide detailed temporal information about each accident, which can be crucial for analyzing trends over time, understanding seasonal variations, or exploring the distribution of accidents by day of the week. The enriched dataset (acc_data) will facilitate deeper temporal analyses and visualizations related to road traffic accidents.\n\nacc_data &lt;- BMR_acc_data %&gt;%\n  mutate(\n    incident_date = as.Date(incident_datetime),\n    year = year(incident_datetime),\n    month = month(incident_datetime, label = TRUE),\n    day_of_week = wday(incident_datetime, label = TRUE)\n  )"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-the-number-of-accidents-over-time",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-the-number-of-accidents-over-time",
    "title": "Take-Home_Exercise 1",
    "section": "4.2 Plotting the Number of Accidents Over Time",
    "text": "4.2 Plotting the Number of Accidents Over Time\nThis code creates a histogram to visualize the number of road accidents over time, with each bar representing the number of accidents occurring within a 30-day bin width:\n\nggplot(): Initializes the plot, mapping incident_date to the x-axis (aes(x = incident_date)), representing the timeline of accidents.\ngeom_histogram(): Adds a histogram layer with a bin width of 30 days. The bars are filled with blue (fill = \"blue\") and outlined in white (color = \"white\").\nlabs(): Adds labels to the plot, setting the title to \"Number of Accidents Over Time\" and labeling the x-axis as \"Date\" and the y-axis as \"Number of Accidents\".\n\nThis visualization allows you to observe the distribution of accidents over time, identifying periods of high accident frequency and any temporal patterns.\n\n# Plot number of accidents by year and month\nggplot(acc_data, aes(x = incident_date)) +\n  geom_histogram(binwidth = 30, fill = \"blue\", color = \"white\") +\n  labs(title = \"Number of Accidents Over Time\", x = \"Date\", y = \"Number of Accidents\")\n\n\n\n\n\n\n\n\nThis histogram shows the number of road accidents over time from 2019 to early 2023. Each bar represents the number of accidents occurring over a 30-day period.\n\nObservations:\n\nThere is significant variability in the number of accidents over time, with some months experiencing over 400 accidents.\nA noticeable peak occurs around early 2020, followed by a dip, possibly reflecting effects of external events like the COVID-19 pandemic on traffic volume and accidents.\nAccident counts seem to rise again towards the end of 2022 and early 2023.\n\nThis visualization provides an initial overview of how accident frequencies change over time, and it can be used to identify trends or investigate specific periods further."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-the-spatial-distribution-of-accidents-on-the-road-network",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-the-spatial-distribution-of-accidents-on-the-road-network",
    "title": "Take-Home_Exercise 1",
    "section": "4.3 Plotting the Spatial Distribution of Accidents on the Road Network",
    "text": "4.3 Plotting the Spatial Distribution of Accidents on the Road Network\nThis code visualizes the spatial distribution of road accidents in the Bangkok Metropolitan Region (BMR) by overlaying the road network and accident points on a map using ggplot() and geom_sf():\n\ngeom_sf() is used to plot spatial objects, where:\n\nBangkok roads are plotted in antiquewhite.\nNonthaburi roads are plotted in aquamarine.\nNakhon Pathom roads are plotted in azure.\nPathum Thani roads are plotted in burlywood.\nSamut Prakan roads are plotted in chartreuse.\nSamut Sakhon roads are plotted in gray.\n\nRoad accidents from the BMR_acc_data dataset are plotted on top of the road network in red with a size of 0.5 to represent individual accident points.\nThe title of the plot is set to “Spatial Distribution of Road Accidents in BMR”.\n\nThis plot will display the accident locations overlaid on the road network of the different provinces within the BMR, helping to visually identify accident-prone areas or road segments with frequent accidents.\n\n# Plot the spatial distribution of accidents on the road network\nggplot() +\n  geom_sf(data = Bangkok_roads, color = \"antiquewhite\") +\n  geom_sf(data = Nonthaburi_roads, color =\"aquamarine\") +\n  geom_sf(data = Nakhon_Pathom_roads, color = \"azure\") +\n  geom_sf(data = Pathum_Thani_roads, color = \"burlywood\") +\n  geom_sf(data = Samut_Prakan_roads, color = \"chartreuse\") +\n  geom_sf(data = Samut_Sakhon_roads, color = \"gray\") +\n  geom_sf(data = BMR_acc_data, aes(geometry = geometry), color = \"red\", size = 0.5) +\n  labs(title = \"Spatial Distribution of Road Accidents in BMR\")\n\n\n\n\n\n\n\n\nThe map you’ve generated provides a visual representation of the spatial distribution of road accidents in the Bangkok Metropolitan Region (BMR), with the road networks of different provinces overlaid in various colors:\n\nBangkok roads are plotted in antiquewhite.\nNonthaburi roads are in aquamarine.\nNakhon Pathom roads are in azure.\nPathum Thani roads are in burlywood.\nSamut Prakan roads are in chartreuse.\nSamut Sakhon roads are in gray.\n\nThe red points represent the locations of road accidents across the region, clearly highlighting accident clusters and areas with high traffic incidents. This visualization allows you to quickly identify road segments with frequent accidents, as well as compare accident density across different provinces in BMR."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#creating-a-heatmap-of-accidents-by-hour-of-day-and-day-of-the-week",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#creating-a-heatmap-of-accidents-by-hour-of-day-and-day-of-the-week",
    "title": "Take-Home_Exercise 1",
    "section": "4.4 Creating a Heatmap of Accidents by Hour of Day and Day of the Week",
    "text": "4.4 Creating a Heatmap of Accidents by Hour of Day and Day of the Week\nThis code generates a heatmap to visualize the distribution of road accidents based on the hour of the day and the day of the week.\n\nStep 1: Extract Hour of Day and Day of Week\n\nThe mutate() function is used to create two new columns in acc_data:\n\nhour_of_day: Extracts the hour from incident_datetime using the hour() function.\nday_of_week: Extracts the day of the week using wday(label = TRUE), with labels for easier interpretation.\n\n\n\n\nStep 2: Summarize Data by Hour and Day\n\nThe data is grouped by both hour_of_day and day_of_week using group_by(), and the number of accidents is counted for each combination using summarise().\n\n\n\nStep 3: Create the Heatmap\n\nThe heatmap is created using ggplot(), where:\n\nx-axis: Represents the hour_of_day.\ny-axis: Represents the day_of_week.\nfill: Represents the count of accidents for each hour-day combination.\n\ngeom_tile(): Creates the heatmap tiles, where each tile’s color represents the accident count.\nscale_fill_gradient(): The color gradient is set from white (low count) to red (high count) to emphasize areas with more accidents.\nlabs(): Sets the title and axis labels.\ntheme_minimal(): Applies a minimal theme for a clean look.\n\nThis heatmap provides insights into the busiest accident times during the week, highlighting patterns such as peak hours or specific days with a high frequency of accidents.\n\n# Step 1: Extract hour of the day and day of the week for accidents\nacc_data &lt;- acc_data %&gt;%\n  mutate(hour_of_day = hour(incident_datetime),\n         day_of_week = wday(incident_datetime, label = TRUE))\n\n# Step 2: Summarize the data by hour of day and day of week\nsummary_data &lt;- acc_data %&gt;%\n  group_by(hour_of_day, day_of_week) %&gt;%\n  summarise(count = n())  # Count the number of accidents for each combination\n\n# Step 3: Create a heatmap of accidents by hour and day of week\nggplot(summary_data, aes(x = hour_of_day, y = day_of_week, fill = count)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  labs(title = \"Heatmap of Accidents by Hour of Day and Day of Week\", \n       x = \"Hour of Day\", y = \"Day of Week\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis heatmap visualizes the number of road accidents across different hours of the day and days of the week. The intensity of the color represents the accident frequency, with darker red areas indicating higher counts.\n\n\nObservations:\n\nPeak Accident Times: There are two notable peaks, one in the late morning around 10 AM and another during the evening hours around 8 PM.\nDay of the Week: Accidents tend to occur consistently throughout the week, with Friday (周五) and Saturday (周六) having slightly higher accident counts in the evening compared to other days.\nEarly Morning: Fewer accidents are recorded during the early morning hours (midnight to around 6 AM), as indicated by the lighter shading."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#extracting-the-month-from-the-incident-date",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#extracting-the-month-from-the-incident-date",
    "title": "Take-Home_Exercise 1",
    "section": "4.5 Extracting the Month from the Incident Date",
    "text": "4.5 Extracting the Month from the Incident Date\nIn this step, the code is extracting the month from the incident_datetime column in the BMR_acc_data dataset and creating a new column called month. The mutate() function is used to add this new variable, where the month() function extracts the month component from the incident_datetime.\nThis transformation allows for further analysis of accident trends by month, helping to explore any seasonal patterns in road accidents.\n\nacc_data &lt;- BMR_acc_data %&gt;%\n  mutate(month = month(incident_datetime))  # Extract month from incident datetime"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-monthly-accident-counts-over-time",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#plotting-monthly-accident-counts-over-time",
    "title": "Take-Home_Exercise 1",
    "section": "4.6 Plotting Monthly Accident Counts Over Time",
    "text": "4.6 Plotting Monthly Accident Counts Over Time\nThis code creates a time series plot to visualize the monthly number of road accidents over time:\n\nStep 1: Extract Year and Month\n\nThe mutate() function is used to extract both the year and month from the incident_datetime column, creating new columns year and month.\n\n\n\nStep 2: Summarize Data by Year and Month\n\nThe group_by() function groups the data by year and month, and the summarise() function counts the number of accidents for each month, creating the monthly_accidents dataset.\n\n\n\nStep 3: Plot the Time Series\n\nggplot(): Initializes the plot with:\n\nx-axis: The combination of year and month using the interaction() function.\ny-axis: The monthly accident count.\n\ngeom_line(): Adds a blue line connecting the accident counts over time.\ngeom_point(): Adds points on the line to mark each month’s accident count.\nlabs(): Adds a title and labels the axes (“Year-Month” for the x-axis and “Number of Accidents” for the y-axis).\ntheme(): Rotates the x-axis text to make it more readable, setting the angle to 90 degrees.\n\nThis time series plot provides a clear visualization of monthly accident trends, allowing for the identification of any long-term patterns or fluctuations in road accident counts over the years.\n\nacc_data &lt;- acc_data %&gt;%\n  mutate(\n    year = year(incident_datetime),    # Extract year\n    month = month(incident_datetime)   # Extract month\n  )\nmonthly_accidents &lt;- acc_data %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(count = n())\n# Plot time series of accidents\nggplot(monthly_accidents, aes(x = interaction(year, month, sep = \"-\"), y = count, group = 1)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  labs(title = \"Monthly Accident Counts Over Time\", x = \"Year-Month\", y = \"Number of Accidents\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\nThis time series plot shows the monthly accident counts over the period from January 2019 to December 2022.\n\n\nObservations:\n\nFluctuations: The number of accidents fluctuates throughout the period, with some peaks reaching around 350 accidents in certain months.\nTrend: There seems to be a general upward trend in accidents toward the end of the time series, especially after a noticeable dip in late 2021.\nNotable Events: There is a sharp decline in accident counts around late 2021, followed by a rapid increase. This could be associated with external factors such as reduced traffic during lockdown periods or other significant events."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#seasonal-decomposition-of-monthly-accident-counts",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#seasonal-decomposition-of-monthly-accident-counts",
    "title": "Take-Home_Exercise 1",
    "section": "4.7 Seasonal Decomposition of Monthly Accident Counts",
    "text": "4.7 Seasonal Decomposition of Monthly Accident Counts\nThis code performs seasonal decomposition on the time series of monthly accident counts, which helps to break down the data into its trend, seasonal, and residual components.\n\nStep 1: Create a Time Series Object\n\nThe ts() function is used to create a time series object ts_data for the monthly accident counts.\n\nThe count column from the monthly_accidents dataset is used as the data.\nThe start argument defines the starting year and month for the time series.\nfrequency = 12 indicates that the data is monthly (12 periods per year).\n\n\n\n\nStep 2: Perform Seasonal Decomposition\n\nThe stl() function performs Seasonal-Trend decomposition using LOESS (STL) on the time series ts_data.\n\ns.window = \"periodic\" specifies that the seasonal component is to be extracted with a fixed periodicity (i.e., 12 months for monthly data).\n\n\n\n\nStep 3: Plot the Decomposition\n\nThe plot() function visualizes the decomposition, showing the trend, seasonality, and residuals of the monthly accident data.\n\nThe main title is set to “Seasonal Decomposition of Monthly Accident Counts.”\n\n\nThis decomposition helps to analyze the underlying patterns in the data, revealing any long-term trends, recurring seasonal effects (e.g., certain months having more accidents), and any random residuals or noise that cannot be explained by the trend or seasonality.\n\n# Create a time series object for accident counts\nts_data &lt;- ts(monthly_accidents$count, start = c(min(monthly_accidents$year), min(as.numeric(monthly_accidents$month))), frequency = 12)\n\n# Perform seasonal decomposition\ndecomp &lt;- stl(ts_data, s.window = \"periodic\")\n\n# Plot the decomposition\nplot(decomp, main = \"Seasonal Decomposition of Monthly Accident Counts\")\n\n\n\n\n\n\n\n\nThis plot displays the seasonal decomposition of monthly accident counts, breaking the data down into the following components:\n\nData (Top Panel): The original time series showing the observed monthly accident counts.\nSeasonal Component (Second Panel): This reveals recurring patterns in the data, which repeat every 12 months (yearly cycle). You can observe consistent seasonal fluctuations, with certain months regularly showing higher or lower accident counts.\nTrend Component (Third Panel): The long-term trend in the data shows an initial decline from 2019 through mid-2021, followed by a notable increase in accident counts from late 2021 into 2023.\nRemainder (Residuals) (Bottom Panel): The residuals represent the remaining variation in the data after removing the trend and seasonal components. These fluctuations are likely due to random noise or other factors not captured by the model.\n\n\n\nObservations:\n\nThe seasonal component shows a regular pattern of variation across the year, with higher accidents during certain months.\nThe trend shows a decrease during 2020 and early 2021, possibly due to reduced traffic during COVID-19 lockdowns, followed by a sharp increase starting in late 2021.\nThe residuals appear relatively small and random, suggesting that the model captures the key patterns well."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monthly-spatial-analysis-of-road-accident-hotspots-in-bmr-using-kde",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monthly-spatial-analysis-of-road-accident-hotspots-in-bmr-using-kde",
    "title": "Take-Home_Exercise 1",
    "section": "4.8 Monthly Spatial Analysis of Road Accident Hotspots in BMR Using KDE",
    "text": "4.8 Monthly Spatial Analysis of Road Accident Hotspots in BMR Using KDE\nThe code performs Kernel Density Estimation (KDE) on road accident data for each month and combines the results into a single data frame for further analysis or visualization.\n\nKey Steps:\n\nKDE Calculation for Each Month:\n\nThe kde_for_month function extracts spatial coordinates (st_coordinates()) from accident data and creates a bounding box (as.owin()) for defining the spatial window.\nIt converts the accident data into a point pattern (ppp) and applies Kernel Density Estimation (density()) with a smoothing parameter sigma = 300.\nThe result is transformed into a data frame (as.data.frame()) to be used for visualization with ggplot.\n\nSuppress Duplicate Warnings:\n\nWarnings related to duplicate points in the dataset are suppressed to avoid unnecessary output.\n\nApply KDE Grouped by Month:\n\nThe KDE function is applied to each month’s data using group_by(Month_num) and group_map(). This groups the dataset by month and applies the kde_for_month function to each group.\n\nCombining KDE Results:\n\nThe resulting KDE for each month is combined into a single data frame (kde_combined) using bind_rows(), with an additional month column to identify the corresponding month for each KDE result.\n\n\n\nkde_for_month &lt;- function(month_data) {\n  # Extract coordinates\n  coords &lt;- st_coordinates(month_data$geometry)\n  \n  # No need to remove duplicates, proceed with all coordinates\n  # Create a bounding box or window for the KDE\n  window &lt;- as.owin(st_bbox(month_data))\n  \n  # Convert to a point pattern for KDE\n  acc_ppp &lt;- ppp(x = coords[,1], y = coords[,2], window = window)\n  \n  # Perform Kernel Density Estimation with an optimal sigma value\n  acc_kde &lt;- density(acc_ppp, sigma = 300)  # Adjust sigma as needed\n  \n  # Convert KDE result to data frame for ggplot\n  kde_raster &lt;- as.data.frame(acc_kde)\n  \n  return(kde_raster)\n}\n\n# Suppress warnings related to duplicate points\noptions(warn = -1)\n\n# Calculate KDE for each month\nkde_data &lt;- BMR_acc_data %&gt;%\n  group_by(Month_num) %&gt;%\n  group_map(~ kde_for_month(.x), .keep = TRUE)  # Apply KDE for each group (month)\n\n# Combine all KDE data\nkde_combined &lt;- bind_rows(kde_data, .id = \"month\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monthly-heatmap-of-road-accident-densities-in-bmr-using-kde",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monthly-heatmap-of-road-accident-densities-in-bmr-using-kde",
    "title": "Take-Home_Exercise 1",
    "section": "4.9 Monthly Heatmap of Road Accident Densities in BMR Using KDE",
    "text": "4.9 Monthly Heatmap of Road Accident Densities in BMR Using KDE\nThis code creates a heatmap to visualize the Kernel Density Estimation (KDE) of road traffic accidents for each month, using ggplot2.\n\nKey Elements:\n\nggplot with Raster Layer (geom_raster()):\n\ngeom_raster() is used to plot the KDE results, where each tile is filled based on the density values.\n\nColor Scale (scale_fill_viridis_c()):\n\nThe magma color palette is used with a reversed direction for better contrast, visually highlighting high-density areas.\nThe fill = value aesthetic maps density values to color intensity.\n\nFaceted by Month (facet_wrap()):\n\nThe KDE results are faceted by month using facet_wrap(~ month), arranging the plots into a grid with 3 columns (ncol = 3).\n\nMinimal Theme (theme_minimal()):\n\nA minimalist design is applied, removing axis labels, ticks, and grid lines for a clean visual focus on the KDE.\n\nCustom Labels:\n\nThe plot title is set as “KDE of Road Traffic Accidents by Month”, and the fill color legend is labeled as “Density”.\n\n\n\nggplot(kde_combined, aes(x = x, y = y, fill = value)) +\n  geom_raster() +\n  scale_fill_viridis_c(option = \"magma\", direction = -1) +  # Magma with reverse direction for contrast\n  labs(title = \"KDE of Road Traffic Accidents by Month\", fill = \"Density\") +\n  facet_wrap(~ month, ncol = 3) +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nGeneral Overview:\n\nThe heatmap panels display the spatial distribution of road accident densities across each month in the Bangkok Metropolitan Region (BMR).\nThe intensity of red indicates high-density areas where accidents are more frequent, while yellow represents low-density areas.\nThe color scale on the right, ranging from 0 to around 2e-05, shows the density values, with darker regions indicating more accident-prone areas.\n\nConsistent Hotspots:\n\nThere appear to be consistent accident hotspots across multiple months, especially around key intersections and high-traffic zones in central regions.\nThese hotspots are visible in nearly all months (e.g., months 1, 4, 7, and 12), suggesting persistent high-risk zones in the BMR, which could be due to road layout, traffic volume, or hazardous conditions.\n\nSeasonal Variation:\n\nAlthough the overall pattern of accident distribution seems similar across months, the intensity of hotspots may vary slightly between certain months.\nSome months, such as October (10) and December (12), show slightly more intense hotspot activity than others, potentially correlating with changes in traffic patterns due to holidays or seasonal weather conditions.\n\nLow Density Areas:\n\nA significant portion of the BMR has very low accident densities, as indicated by the vast yellow regions on the heatmap.\nThese areas may represent residential or less congested zones with lower accident frequencies, or areas with fewer road users during certain months.\n\nImplications for Traffic Management:\n\nThe identification of consistent accident hotspots suggests areas where increased traffic enforcement, road safety measures, or infrastructure improvements may be necessary.\nAuthorities can focus on these high-density zones for further investigation into accident causes and potential safety interventions.\n\nFurther Investigation:\n\nThe variations in density between months could be explored further by comparing external factors, such as weather patterns, road conditions, and holiday seasons, to see if these contribute to the observed changes in accident density."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#preparing-spatial-accident-data-for-point-pattern-analysis-in-bmr",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#preparing-spatial-accident-data-for-point-pattern-analysis-in-bmr",
    "title": "Take-Home_Exercise 1",
    "section": "4.10 Preparing Spatial Accident Data for Point Pattern Analysis in BMR",
    "text": "4.10 Preparing Spatial Accident Data for Point Pattern Analysis in BMR\nThis code prepares the road accident data for spatial point pattern analysis by converting it into a format compatible with the spatstat package in R. Here’s a breakdown of the key steps:\n\nSuppress Warnings:\n\noptions(warn = -1) disables warnings related to duplicate points, which may occur when dealing with spatial data.\n\nExtract Spatial Coordinates:\n\nst_coordinates(acc_data$geometry) extracts the spatial coordinates (longitude and latitude) from the geometry column in the acc_data dataset.\n\nCreate a Window (Bounding Box):\n\nas.owin(st_bbox(acc_data)) creates a bounding box around the study area, which will be used as the spatial window for the analysis. The bounding box is converted into an observation window using as.owin().\n\nConvert to Point Pattern (ppp):\n\nThe accident data is converted into a point pattern object using the ppp() function, where the x and y coordinates (longitude and latitude) are input along with the study area’s window. This format is essential for conducting further spatial analyses, such as Kernel Density Estimation (KDE) or spatial clustering.\n\n\n\noptions(warn = -1)\n# Extract the spatial coordinates from the geometry column\ncoords &lt;- st_coordinates(acc_data$geometry)\n\n# Create a window (bounding box) for your study area\nwindow &lt;- as.owin(st_bbox(acc_data))\n\n# Convert the accident data to a point pattern (ppp) object for spatstat\nacc_ppp &lt;- ppp(x = coords[, 1], y = coords[, 2], window = window)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#analyzing-spatial-clustering-of-road-accidents-using-ripleys-k-function",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#analyzing-spatial-clustering-of-road-accidents-using-ripleys-k-function",
    "title": "Take-Home_Exercise 1",
    "section": "4.11 Analyzing Spatial Clustering of Road Accidents Using Ripley’s K-function",
    "text": "4.11 Analyzing Spatial Clustering of Road Accidents Using Ripley’s K-function\nThis code performs a K-function analysis (Ripley’s K-function) on road accident data, which is useful for understanding the spatial distribution of accidents and identifying clustering patterns. Here’s a breakdown:\n\nK-function Calculation:\n\nKest(acc_ppp) computes Ripley’s K-function on the point pattern object acc_ppp. The K-function measures the spatial clustering of points over a range of distances, helping to determine whether points (accidents) are clustered, randomly distributed, or dispersed at different scales.\n\nPlot the K-function:\n\nThe plot() function visualizes the result of the K-function analysis.\nThe plot’s title is set as “Ripley’s K-function for Road Accidents”, and the legend is positioned in the top right using legendargs = list(x = \"topright\").\n\n\n\nInterpretation:\n\nThe K-function plot will show how accident clustering behaves at different distances. If the K-function line is above the expected value for a random distribution, it indicates clustering. If it’s below, it suggests dispersion.\n\n\n# Perform K-function analysis\nk_function &lt;- Kest(acc_ppp)\n\n# Plot the result\nplot(k_function, main = \"Ripley's K-function for Road Accidents\", \n     legendargs = list(x = \"topright\"))\n\n\n\n\n\n\n\n\nThe plot illustrates Ripley’s K-function, which analyzes the spatial distribution of road accidents and helps determine whether they are clustered, randomly distributed, or dispersed over space.\n\nKey Observations:\n\nBlack Line (Observed K-function):\n\nThe black line represents the K-function for the actual accident data.\nA rapid increase in the K-function suggests that road accidents are highly clustered.\nThe clustering effect is particularly noticeable at smaller distances and persists as the distance increases.\n\nRed Dashed Line (Poisson Process):\n\nThe red dashed line represents the expected K-function if the accidents were randomly distributed (Poisson process).\nThe observed K-function (black line) is consistently above the Poisson process, which indicates that road accidents are not randomly distributed but are spatially clustered.\n\nDistance:\n\nAs the distance r increases (on the x-axis), the K-function continues to rise sharply, showing that clustering occurs over both small and larger spatial scales.\nThis suggests that there are accident-prone zones where road accidents occur close to one another, and this clustering persists over extended areas.\n\n\n\n\nConclusion:\nRipley’s K-function confirms that road accidents are spatially clustered in the study area. The clustering is evident at both small and large distances, indicating the presence of high-risk zones where accidents are more frequent. Understanding this spatial clustering is essential for developing targeted safety interventions, such as traffic control measures or infrastructure improvements in accident hotspots."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monte-carlo-simulation-for-validating-road-accident-clustering-using-ripleys-k-function",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#monte-carlo-simulation-for-validating-road-accident-clustering-using-ripleys-k-function",
    "title": "Take-Home_Exercise 1",
    "section": "4.12 Monte Carlo Simulation for Validating Road Accident Clustering Using Ripley’s K-function",
    "text": "4.12 Monte Carlo Simulation for Validating Road Accident Clustering Using Ripley’s K-function\nThis code performs Monte Carlo simulations to generate an envelope for Ripley’s K-function. The envelope is used to compare the observed clustering pattern of road accidents with a Complete Spatial Randomness (CSR) pattern, which helps in determining if the observed clustering is statistically significant.\n\nKey Steps:\n\nMonte Carlo Simulations:\n\nThe envelope() function is applied to the point pattern acc_ppp with the Kest function. This function generates a range of expected K-functions based on 99 random simulations (nsim = 99), assuming the accidents are distributed according to CSR.\nCSR serves as a baseline or null hypothesis, which assumes that the points (road accidents) are randomly distributed in space.\n\nK-function Envelope:\n\nThe result (envelope_k) is an envelope that includes a lower and upper bound based on the Monte Carlo simulations. The observed K-function can be compared against this envelope to assess whether the clustering pattern is significantly different from randomness.\n\n\n\n\nInterpretation:\n\nIf the observed K-function lies outside the envelope, this indicates that the observed spatial pattern is significantly different from what would be expected under CSR, suggesting significant clustering or dispersion.\n\n\n# Perform Monte Carlo simulations to compare with CSR\nenvelope_k &lt;- envelope(acc_ppp, Kest, nsim = 99)  # Perform 99 simulations for CSR\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n# Plot the envelope of the K-function (observed vs CSR)\nplot(envelope_k, main = \"K-function with CSR Envelope\")\n\n\n\n\n\n\n\n\nThis plot shows the comparison between the observed K-function for road accidents and the CSR envelope generated from Monte Carlo simulations. The plot helps in assessing whether the observed spatial pattern of road accidents is significantly different from a random distribution.\n\n\nKey Components:\n\nBlack Line (K_obs(r)):\n\nThis represents the observed K-function for the road accident data.\nThe line shows how road accidents are spatially clustered at various distances, with larger values of r on the x-axis corresponding to greater distances between points.\n\nDashed Red Line (K_theo(r)):\n\nThis represents the theoretical K-function under Complete Spatial Randomness (CSR). It serves as a baseline to compare the observed clustering against a random spatial distribution of points.\n\nGray Shaded Envelope (K_hi(r) and K_lo(r)):\n\nThe envelope is the range of expected values from the Monte Carlo simulations (99 simulations).\nIf the observed K-function (black line) lies outside this envelope, it suggests that the observed pattern is significantly different from randomness.\n\n\n\n\nInterpretation:\n\nThe observed K-function (black line) is consistently above the upper boundary of the CSR envelope, indicating significant spatial clustering of road accidents.\nThis suggests that the clustering of road accidents is not due to random chance but likely due to underlying factors such as road conditions, traffic patterns, or intersection density.\nThe K-function continues to rise steeply as the distance increases, showing that the clustering persists over larger distances."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#geospatial-analysis-of-bangkok-road-accidents.",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#geospatial-analysis-of-bangkok-road-accidents.",
    "title": "Take-Home_Exercise 1",
    "section": "5.1 Geospatial analysis of Bangkok road accidents.",
    "text": "5.1 Geospatial analysis of Bangkok road accidents.\nthree datasets are assigned to variables:\n\nbangkok_acc_data: Accident data for the Bangkok Metropolitan Region, assigned from BMR_acc_data.\nbangkok_boundary: Geographic boundary data for Bangkok.\nbangkok_roads: Road network data for Bangkok.\n\n\n# Use your dataset names\nbangkok_acc_data &lt;- BMR_acc_data\nbangkok_boundary &lt;- bangkok_boundary\nbangkok_roads &lt;- Bangkok_roads"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#crs-transformation-for-spatial-data-in-road-accident-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#crs-transformation-for-spatial-data-in-road-accident-analysis",
    "title": "Take-Home_Exercise 1",
    "section": "5.2 CRS Transformation for Spatial Data in Road Accident Analysis",
    "text": "5.2 CRS Transformation for Spatial Data in Road Accident Analysis\nThe code shown here is performing a coordinate reference system (CRS) transformation for spatial datasets related to road accidents and road networks in Bangkok. This step ensures that all spatial data is using the same CRS for accurate analysis and visualization.\n\nKey Steps:\n\nTarget CRS (32647):\n\nThe target CRS is set to UTM Zone 47N (EPSG: 32647), which is appropriate for spatial data in Bangkok, ensuring accurate distance and area calculations.\n\nCRS Transformation (st_transform):\n\nThe function st_transform() is used to project each spatial dataset into the specified CRS:\n\nbangkok_acc_data: Road accident data for Bangkok.\nbangkok_roads: Road network data for Bangkok.\nbangkok_boundary: Administrative boundary data for Bangkok.\n\n\n\nThis transformation is necessary because spatial datasets often come with different CRS, and for accurate geospatial analysis (e.g., distance measurements, KDE), all datasets must be in the same projection.\n\ntarget_crs &lt;- 32647  # UTM Zone 47N\nbangkok_acc_data &lt;- st_transform(bangkok_acc_data, crs = target_crs)\nbangkok_roads &lt;- st_transform(bangkok_roads, crs = target_crs)\nbangkok_boundary &lt;- st_transform(bangkok_boundary, crs = target_crs)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#preprocessing-road-network-data-filtering-and-geometry-casting-in-geospatial-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#preprocessing-road-network-data-filtering-and-geometry-casting-in-geospatial-analysis",
    "title": "Take-Home_Exercise 1",
    "section": "5.3 Preprocessing Road Network Data: Filtering and Geometry Casting in Geospatial Analysis",
    "text": "5.3 Preprocessing Road Network Data: Filtering and Geometry Casting in Geospatial Analysis\nThis code snippet processes the road network data for Bangkok, ensuring that only the relevant geometry types are retained and properly formatted for further geospatial analysis. Here’s a breakdown:\n\nFilter Geometry Types:\n\nThe first part of the code filters the road network dataset (bangkok_roads) to retain only the LINESTRING and MULTILINESTRING geometries. These geometries represent roads as lines and multi-segmented lines, which are essential for spatial analysis in road networks.\n\nCast to LINESTRING:\n\nThe second part of the code converts or “casts” any MULTILINESTRING geometries into LINESTRING geometries using st_cast(). This ensures consistency, as most road network analysis requires uniform geometry types.\nThe argument group_or_split = TRUE ensures that complex MULTILINESTRING geometries are split into individual LINESTRING components where necessary.\n\n\n\nroads_in_bangkok &lt;- bangkok_roads %&gt;%\n  filter(st_geometry_type(bangkok_roads) %in% c(\"LINESTRING\", \"MULTILINESTRING\"))\nroads_in_bangkok &lt;- st_cast(roads_in_bangkok, \"LINESTRING\", group_or_split = TRUE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#dividing-and-sampling-road-networks-lixelization-and-center-extraction-for-spatial-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#dividing-and-sampling-road-networks-lixelization-and-center-extraction-for-spatial-analysis",
    "title": "Take-Home_Exercise 1",
    "section": "5.4 Dividing and Sampling Road Networks: Lixelization and Center Extraction for Spatial Analysis",
    "text": "5.4 Dividing and Sampling Road Networks: Lixelization and Center Extraction for Spatial Analysis\nThis code is part of preparing the road network data for spatial analysis by dividing the road network into lixels (linear pixels) and extracting their central points. Lixels are a crucial concept in network-based spatial analysis, particularly for road networks.\n\nCreate Lixels:\n\nlixelize_lines() divides the road network (roads_in_bangkok) into segments called lixels.\nThe argument 10000 specifies the desired length of each lixel in meters.\nThe argument mindist = 5000 ensures a minimum distance of 5000 meters between the lixels to avoid overlap and maintain spatial resolution.\n\nExtract Lixel Centers:\n\nlines_center() extracts the central points of the lixels created in the previous step, storing them in samples_bangkok. These central points are often used as sample points for further spatial analyses, such as KDE or spatial interpolation, in road network studies.\n\n\n\nlixels_bangkok &lt;- lixelize_lines(roads_in_bangkok, 10000, mindist = 5000)\nsamples_bangkok &lt;- lines_center(lixels_bangkok)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#network-kernel-density-estimation-nkde-for-road-accident-analysis-with-parallel-processing",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#network-kernel-density-estimation-nkde-for-road-accident-analysis-with-parallel-processing",
    "title": "Take-Home_Exercise 1",
    "section": "5.5 Network Kernel Density Estimation (NKDE) for Road Accident Analysis with Parallel Processing",
    "text": "5.5 Network Kernel Density Estimation (NKDE) for Road Accident Analysis with Parallel Processing\nThis code performs Network Kernel Density Estimation (NKDE) on the road network using parallel processing for faster computation. NKDE is used to estimate the density of events (e.g., accidents) over a road network, considering the underlying spatial structure of the network.\n\nKey Steps:\n\nParallel Processing Setup:\n\nplan(multisession) enables parallel computation using multiple CPU cores, which speeds up the NKDE process by distributing tasks across different processing units.\n\nNKDE Calculation (nkde()):\n\nlines = lixels_bangkok: The road network (lixels) where the events (accidents) occur.\nevents = bangkok_acc_data: The event data, in this case, road accidents in Bangkok.\nw = rep(1, nrow(bangkok_acc_data)): Assigns equal weight to each accident.\nsamples = samples_bangkok: Central points from the lixels used as sampling locations for estimating density.\nkernel_name = “quartic”: The quartic kernel is used for smoothing the density estimation.\nbw = 1000: Bandwidth (in meters), which controls the smoothing scale of the kernel density estimation.\nmethod = “simple”: A simplified method for NKDE, suitable for this analysis.\ngrid_shape = c(100, 100): Specifies the grid resolution for the density estimation.\nverbose = TRUE: Enables detailed output during computation to track progress.\n\nFuture Processing:\n\nfuture() is used to parallelize the NKDE computation. The result is stored in future_nkde_result, allowing the process to run in the background and use multiple cores efficiently.\n\n\n\nplan(multisession)  # Parallelize using multiple CPU cores\nfuture_nkde_result &lt;- future({\n    nkde(\n        lines = lixels_bangkok,\n        events = bangkok_acc_data,\n        w = rep(1, nrow(bangkok_acc_data)),\n        samples = samples_bangkok,\n        kernel_name = \"quartic\",\n        bw = 1000,\n        div = \"bw\",\n        method = \"simple\",\n        grid_shape = c(100, 100),\n        verbose = TRUE\n    )\n})\n\n\nprint(future_nkde_result)\n\nMultisessionFuture:\nLabel: '&lt;none&gt;'\nExpression:\n{\n    nkde(lines = lixels_bangkok, events = bangkok_acc_data, w = rep(1, \n        nrow(bangkok_acc_data)), samples = samples_bangkok, kernel_name = \"quartic\", \n        bw = 1000, div = \"bw\", method = \"simple\", grid_shape = c(100, \n            100), verbose = TRUE)\n}\nLazy evaluation: FALSE\nAsynchronous evaluation: TRUE\nLocal evaluation: TRUE\nEnvironment: R_GlobalEnv\nCapture standard output: TRUE\nCapture condition classes: 'condition' (excluding 'nothing')\nGlobals: 3 objects totaling 205.19 MiB (sf 'lixels_bangkok' of 109.38 MiB, sf 'bangkok_acc_data' of 5.54 MiB, sf 'samples_bangkok' of 90.27 MiB)\nPackages: 1 packages ('spNetwork')\nL'Ecuyer-CMRG RNG seed: &lt;none&gt; (seed = FALSE)\nResolved: FALSE\nValue: &lt;not collected&gt;\nConditions captured: &lt;none&gt;\nEarly signaling: FALSE\nOwner process: df53c1dd-823d-a8bd-aeb5-9a0b3dd9086b\nClass: 'MultisessionFuture', 'ClusterFuture', 'MultiprocessFuture', 'Future', 'environment'\n\n\nThe MultisessionFuture object output indicates that the Network Kernel Density Estimation (NKDE) task has been initiated and is running asynchronously across multiple CPU cores. Here’s what the details tell us:\n\n\nKey Details:\n\nAsynchronous Evaluation (TRUE):\n\nThe task is running asynchronously, meaning it is being executed in the background, which allows other processes to run concurrently. The evaluation of the NKDE will be completed independently.\n\nLazy Evaluation (FALSE):\n\nThis indicates that the task has started executing immediately, without waiting for explicit evaluation to be triggered.\n\nResources Used:\n\nMemory Usage: The task involves significant data objects, consuming 205.19 MiB of memory in total.\n\nThe lixels_bangkok object, which represents the road network segments (lixels), is the largest, using about 109.38 MiB.\nThe samples_bangkok object, which contains the sample points (centers of lixels), uses about 90.27 MiB.\nThe bangkok_acc_data object, which stores the accident event data, takes 5.54 MiB.\n\n\nParallel Processing:\n\nThe job is leveraging the spNetwork package to perform the network-based KDE, which is running across multiple sessions (using multicore processing). This significantly improves the computational efficiency, especially for large spatial datasets.\n\nResolved: FALSE:\n\nThe NKDE task has not yet completed, meaning the density estimation process is still in progress. Once resolved, the result will be stored in future_nkde_result and can be accessed or plotted.\n\nTracking Output:\n\nThe setup ensures that any output or conditions (e.g., errors or warnings) will be captured during the NKDE process. This is helpful for debugging and monitoring the execution status.\n\n\n\n\nConclusion:\nThe NKDE computation is actively running in parallel across multiple CPU cores to estimate the density of road accidents over the road network. The use of asynchronous evaluation allows for efficient resource utilization. Given the memory consumption, this approach is well-suited for handling large spatial datasets like road networks and accident data in Bangkok. You can monitor the status, and once completed, visualize the NKDE results to analyze accident density patterns across the road network."
  }
]